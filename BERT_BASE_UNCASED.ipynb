{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_BASE_UNCASED",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dio6jsR0OZHU",
        "outputId": "255700eb-4df7-4414-ccb5-1d31c132249c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 32.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 69.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 53.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "XLyvvfHUTfCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"mixed_domain_godamit.csv\",usecols=[\"data\",\"label\"])"
      ],
      "metadata": {
        "id": "5_Gad5PtTnKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df1=pd.read_csv(\"covid_kd_val.csv\",usecols=[\"data\",\"label\"])\n",
        "df2=pd.read_csv(\"covid_kd_test.csv\",usecols=[\"data\",\"label\"])"
      ],
      "metadata": {
        "id": "JJLECEIRT5Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=df1.sample(frac=1)\n",
        "df1=df1.reset_index()\n",
        "df1.drop(\"index\",axis=1,inplace=True)\n",
        "df2=df2.sample(frac=1)\n",
        "df2=df2.reset_index()\n",
        "df2.drop(\"index\",axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "oTsxQQWirLQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(df1[\"label\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0duPx7stdkD",
        "outputId": "3046352d-44df-4c4c-e7ef-3c0f9bae9b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24355"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int(df.shape[0]*0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMIAJAOXwzSy",
        "outputId": "ad8fa1d7-89f3-4f4e-9a19-59443b12f268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44017"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.sample(frac=1)"
      ],
      "metadata": {
        "id": "GnmiFCPZajro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.reset_index()\n",
        "df.drop(\"index\",axis=1,inplace=True)\n",
        "df=df.loc[:0.25*df.shape[0]]"
      ],
      "metadata": {
        "id": "rkpQhhp-awFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "wLtz5Btga7af",
        "outputId": "53dd8910-57b5-4307-c53a-d307b3cdc003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       level_0                                               data  label\n",
              "0            0  u n make proposals kickstart syrian reform pro...      1\n",
              "1            1         difference year makes eu warms britain may      1\n",
              "2            2  breitbart joel pollack brilliantly shuts view ...      0\n",
              "3            3  trump blasts john mccain lindsey graham focus ...      0\n",
              "4            4  california school bans tag says game rough bre...      0\n",
              "...        ...                                                ...    ...\n",
              "62877    62877  video incredible john bolton hill e mails tell...      0\n",
              "62878    62878  twitter employee briefly shuts trump account p...      1\n",
              "62879    62879  paul ryan wants make healthcare nightmare real...      0\n",
              "62880    62880  federal court rules texas id law violates voti...      0\n",
              "62881    62881              trump attend nato summit brussels may      1\n",
              "\n",
              "[62882 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f1034cb7-eb04-4f9d-8f75-5f20ba68e156\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>level_0</th>\n",
              "      <th>data</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>u n make proposals kickstart syrian reform pro...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>difference year makes eu warms britain may</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>breitbart joel pollack brilliantly shuts view ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>trump blasts john mccain lindsey graham focus ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>california school bans tag says game rough bre...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62877</th>\n",
              "      <td>62877</td>\n",
              "      <td>video incredible john bolton hill e mails tell...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62878</th>\n",
              "      <td>62878</td>\n",
              "      <td>twitter employee briefly shuts trump account p...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62879</th>\n",
              "      <td>62879</td>\n",
              "      <td>paul ryan wants make healthcare nightmare real...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62880</th>\n",
              "      <td>62880</td>\n",
              "      <td>federal court rules texas id law violates voti...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62881</th>\n",
              "      <td>62881</td>\n",
              "      <td>trump attend nato summit brussels may</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>62882 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1034cb7-eb04-4f9d-8f75-5f20ba68e156')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f1034cb7-eb04-4f9d-8f75-5f20ba68e156 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f1034cb7-eb04-4f9d-8f75-5f20ba68e156');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df.loc[:int(0.7*df.shape[0]),'data'],df.loc[:int(0.7*df.shape[0]):,'label'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df.loc[:0.7*int(df.shape[0]),'label'])"
      ],
      "metadata": {
        "id": "KJpFjtl2m_WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_text_cov=df1.loc[:,\"data\"].values\n",
        "val_labels_cov=df1.loc[:,\"label\"].values\n",
        "test_text_cov=df2.loc[:,\"data\"].values\n",
        "test_labels_cov=df2.loc[:,\"label\"].values"
      ],
      "metadata": {
        "id": "0xsukUc-d01z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='bert-base-uncased'\n",
        "bert = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwCAFWwUT8oT",
        "outputId": "610b75ce-e1f2-4079-ac5d-91c557f37a9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at digitalepidemiologylab/covid-twitter-bert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 15\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVbQtPtqUN_U",
        "outputId": "39e9fd8d-8859-41f3-c974-5f1b58404277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text_cov.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text_cov.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKTKA21v6hIG",
        "outputId": "38c250a5-1e84-4ccc-c60a-498c3df332e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels_cov.tolist())\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels_cov.tolist())"
      ],
      "metadata": {
        "id": "3x4reSIkRSZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 4\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "TWX7fE5hUdNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "QfIPcLdIUeCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      super(BERT_Arch, self).__init__()\n",
        "      self.bert = bert \n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.relu =  nn.ReLU()\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc2(x)\n",
        "      x = self.softmax(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "uvbpFGaaUfqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "Ic6hemC3mZ2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERT_Arch(bert)\n",
        "model= model.to(device)"
      ],
      "metadata": {
        "id": "AA-v25Q8UhYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sS6NcD4a8Ib",
        "outputId": "bedbb650-c162-43bc-bc1a-cea8c9073c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(train_labels),\n",
        "                                        y = train_labels                                                    \n",
        "                                    )\n",
        "class_weights = dict(zip(np.unique(train_labels), class_weights))\n",
        "\n",
        "li=[]\n",
        "li.append(class_weights[0])\n",
        "li.append(class_weights[1])\n",
        "print(li)"
      ],
      "metadata": {
        "id": "DYk-C2vlUjLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853bb22e-ac6b-4971-e397-e30445da5963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9870579190158892, 1.0132859773743752]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights"
      ],
      "metadata": {
        "id": "jqZlyonpn3FC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a53a85d-2843-4113-da48-2910b82745db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.9865522540983607, 1: 1.0138194261647802}"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights= torch.tensor(li,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "1t4bIP95Ukwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  total_preds=[]\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    model.zero_grad()        \n",
        "    preds = model(sent_id, mask)\n",
        "    loss = cross_entropy(preds, labels)\n",
        "    total_loss = total_loss + loss.item()\n",
        "    loss.backward()\n",
        "    #gradients are clipped to prevent the exploding gradient problem which is an unusual update in gradients during back propagation \n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "    total_preds.append(preds)\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "wjHqAL9xUk5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate():\n",
        "  print(\"\\nEvaluating...\")\n",
        "  model.eval()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  total_preds = []\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "    batch = [t.to(device) for t in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    with torch.no_grad():\n",
        "      preds = model(sent_id, mask)\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "      total_loss = total_loss + loss.item()\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "      total_preds.append(preds)\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "4iBivJ5vUpe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float('inf')\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "for epoch in range(epochs):\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    train_loss, _ = train()\n",
        "    valid_loss, _ = evaluate()\n",
        "    #weights of the model with the lowest validation loss are saved\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "ClNldL81VKKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a79b9bd2-bede-42a8-9bf3-ba259a9bdb67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of  1,926.\n",
            "  Batch   100  of  1,926.\n",
            "  Batch   150  of  1,926.\n",
            "  Batch   200  of  1,926.\n",
            "  Batch   250  of  1,926.\n",
            "  Batch   300  of  1,926.\n",
            "  Batch   350  of  1,926.\n",
            "  Batch   400  of  1,926.\n",
            "  Batch   450  of  1,926.\n",
            "  Batch   500  of  1,926.\n",
            "  Batch   550  of  1,926.\n",
            "  Batch   600  of  1,926.\n",
            "  Batch   650  of  1,926.\n",
            "  Batch   700  of  1,926.\n",
            "  Batch   750  of  1,926.\n",
            "  Batch   800  of  1,926.\n",
            "  Batch   850  of  1,926.\n",
            "  Batch   900  of  1,926.\n",
            "  Batch   950  of  1,926.\n",
            "  Batch 1,000  of  1,926.\n",
            "  Batch 1,050  of  1,926.\n",
            "  Batch 1,100  of  1,926.\n",
            "  Batch 1,150  of  1,926.\n",
            "  Batch 1,200  of  1,926.\n",
            "  Batch 1,250  of  1,926.\n",
            "  Batch 1,300  of  1,926.\n",
            "  Batch 1,350  of  1,926.\n",
            "  Batch 1,400  of  1,926.\n",
            "  Batch 1,450  of  1,926.\n",
            "  Batch 1,500  of  1,926.\n",
            "  Batch 1,550  of  1,926.\n",
            "  Batch 1,600  of  1,926.\n",
            "  Batch 1,650  of  1,926.\n",
            "  Batch 1,700  of  1,926.\n",
            "  Batch 1,750  of  1,926.\n",
            "  Batch 1,800  of  1,926.\n",
            "  Batch 1,850  of  1,926.\n",
            "  Batch 1,900  of  1,926.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  4,384.\n",
            "  Batch   100  of  4,384.\n",
            "  Batch   150  of  4,384.\n",
            "  Batch   200  of  4,384.\n",
            "  Batch   250  of  4,384.\n",
            "  Batch   300  of  4,384.\n",
            "  Batch   350  of  4,384.\n",
            "  Batch   400  of  4,384.\n",
            "  Batch   450  of  4,384.\n",
            "  Batch   500  of  4,384.\n",
            "  Batch   550  of  4,384.\n",
            "  Batch   600  of  4,384.\n",
            "  Batch   650  of  4,384.\n",
            "  Batch   700  of  4,384.\n",
            "  Batch   750  of  4,384.\n",
            "  Batch   800  of  4,384.\n",
            "  Batch   850  of  4,384.\n",
            "  Batch   900  of  4,384.\n",
            "  Batch   950  of  4,384.\n",
            "  Batch 1,000  of  4,384.\n",
            "  Batch 1,050  of  4,384.\n",
            "  Batch 1,100  of  4,384.\n",
            "  Batch 1,150  of  4,384.\n",
            "  Batch 1,200  of  4,384.\n",
            "  Batch 1,250  of  4,384.\n",
            "  Batch 1,300  of  4,384.\n",
            "  Batch 1,350  of  4,384.\n",
            "  Batch 1,400  of  4,384.\n",
            "  Batch 1,450  of  4,384.\n",
            "  Batch 1,500  of  4,384.\n",
            "  Batch 1,550  of  4,384.\n",
            "  Batch 1,600  of  4,384.\n",
            "  Batch 1,650  of  4,384.\n",
            "  Batch 1,700  of  4,384.\n",
            "  Batch 1,750  of  4,384.\n",
            "  Batch 1,800  of  4,384.\n",
            "  Batch 1,850  of  4,384.\n",
            "  Batch 1,900  of  4,384.\n",
            "  Batch 1,950  of  4,384.\n",
            "  Batch 2,000  of  4,384.\n",
            "  Batch 2,050  of  4,384.\n",
            "  Batch 2,100  of  4,384.\n",
            "  Batch 2,150  of  4,384.\n",
            "  Batch 2,200  of  4,384.\n",
            "  Batch 2,250  of  4,384.\n",
            "  Batch 2,300  of  4,384.\n",
            "  Batch 2,350  of  4,384.\n",
            "  Batch 2,400  of  4,384.\n",
            "  Batch 2,450  of  4,384.\n",
            "  Batch 2,500  of  4,384.\n",
            "  Batch 2,550  of  4,384.\n",
            "  Batch 2,600  of  4,384.\n",
            "  Batch 2,650  of  4,384.\n",
            "  Batch 2,700  of  4,384.\n",
            "  Batch 2,750  of  4,384.\n",
            "  Batch 2,800  of  4,384.\n",
            "  Batch 2,850  of  4,384.\n",
            "  Batch 2,900  of  4,384.\n",
            "  Batch 2,950  of  4,384.\n",
            "  Batch 3,000  of  4,384.\n",
            "  Batch 3,050  of  4,384.\n",
            "  Batch 3,100  of  4,384.\n",
            "  Batch 3,150  of  4,384.\n",
            "  Batch 3,200  of  4,384.\n",
            "  Batch 3,250  of  4,384.\n",
            "  Batch 3,300  of  4,384.\n",
            "  Batch 3,350  of  4,384.\n",
            "  Batch 3,400  of  4,384.\n",
            "  Batch 3,450  of  4,384.\n",
            "  Batch 3,500  of  4,384.\n",
            "  Batch 3,550  of  4,384.\n",
            "  Batch 3,600  of  4,384.\n",
            "  Batch 3,650  of  4,384.\n",
            "  Batch 3,700  of  4,384.\n",
            "  Batch 3,750  of  4,384.\n",
            "  Batch 3,800  of  4,384.\n",
            "  Batch 3,850  of  4,384.\n",
            "  Batch 3,900  of  4,384.\n",
            "  Batch 3,950  of  4,384.\n",
            "  Batch 4,000  of  4,384.\n",
            "  Batch 4,050  of  4,384.\n",
            "  Batch 4,100  of  4,384.\n",
            "  Batch 4,150  of  4,384.\n",
            "  Batch 4,200  of  4,384.\n",
            "  Batch 4,250  of  4,384.\n",
            "  Batch 4,300  of  4,384.\n",
            "  Batch 4,350  of  4,384.\n",
            "\n",
            "Training Loss: 0.553\n",
            "Validation Loss: 0.814\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of  1,926.\n",
            "  Batch   100  of  1,926.\n",
            "  Batch   150  of  1,926.\n",
            "  Batch   200  of  1,926.\n",
            "  Batch   250  of  1,926.\n",
            "  Batch   300  of  1,926.\n",
            "  Batch   350  of  1,926.\n",
            "  Batch   400  of  1,926.\n",
            "  Batch   450  of  1,926.\n",
            "  Batch   500  of  1,926.\n",
            "  Batch   550  of  1,926.\n",
            "  Batch   600  of  1,926.\n",
            "  Batch   650  of  1,926.\n",
            "  Batch   700  of  1,926.\n",
            "  Batch   750  of  1,926.\n",
            "  Batch   800  of  1,926.\n",
            "  Batch   850  of  1,926.\n",
            "  Batch   900  of  1,926.\n",
            "  Batch   950  of  1,926.\n",
            "  Batch 1,000  of  1,926.\n",
            "  Batch 1,050  of  1,926.\n",
            "  Batch 1,100  of  1,926.\n",
            "  Batch 1,150  of  1,926.\n",
            "  Batch 1,200  of  1,926.\n",
            "  Batch 1,250  of  1,926.\n",
            "  Batch 1,300  of  1,926.\n",
            "  Batch 1,350  of  1,926.\n",
            "  Batch 1,400  of  1,926.\n",
            "  Batch 1,450  of  1,926.\n",
            "  Batch 1,500  of  1,926.\n",
            "  Batch 1,550  of  1,926.\n",
            "  Batch 1,600  of  1,926.\n",
            "  Batch 1,650  of  1,926.\n",
            "  Batch 1,700  of  1,926.\n",
            "  Batch 1,750  of  1,926.\n",
            "  Batch 1,800  of  1,926.\n",
            "  Batch 1,850  of  1,926.\n",
            "  Batch 1,900  of  1,926.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  4,384.\n",
            "  Batch   100  of  4,384.\n",
            "  Batch   150  of  4,384.\n",
            "  Batch   200  of  4,384.\n",
            "  Batch   250  of  4,384.\n",
            "  Batch   300  of  4,384.\n",
            "  Batch   350  of  4,384.\n",
            "  Batch   400  of  4,384.\n",
            "  Batch   450  of  4,384.\n",
            "  Batch   500  of  4,384.\n",
            "  Batch   550  of  4,384.\n",
            "  Batch   600  of  4,384.\n",
            "  Batch   650  of  4,384.\n",
            "  Batch   700  of  4,384.\n",
            "  Batch   750  of  4,384.\n",
            "  Batch   800  of  4,384.\n",
            "  Batch   850  of  4,384.\n",
            "  Batch   900  of  4,384.\n",
            "  Batch   950  of  4,384.\n",
            "  Batch 1,000  of  4,384.\n",
            "  Batch 1,050  of  4,384.\n",
            "  Batch 1,100  of  4,384.\n",
            "  Batch 1,150  of  4,384.\n",
            "  Batch 1,200  of  4,384.\n",
            "  Batch 1,250  of  4,384.\n",
            "  Batch 1,300  of  4,384.\n",
            "  Batch 1,350  of  4,384.\n",
            "  Batch 1,400  of  4,384.\n",
            "  Batch 1,450  of  4,384.\n",
            "  Batch 1,500  of  4,384.\n",
            "  Batch 1,550  of  4,384.\n",
            "  Batch 1,600  of  4,384.\n",
            "  Batch 1,650  of  4,384.\n",
            "  Batch 1,700  of  4,384.\n",
            "  Batch 1,750  of  4,384.\n",
            "  Batch 1,800  of  4,384.\n",
            "  Batch 1,850  of  4,384.\n",
            "  Batch 1,900  of  4,384.\n",
            "  Batch 1,950  of  4,384.\n",
            "  Batch 2,000  of  4,384.\n",
            "  Batch 2,050  of  4,384.\n",
            "  Batch 2,100  of  4,384.\n",
            "  Batch 2,150  of  4,384.\n",
            "  Batch 2,200  of  4,384.\n",
            "  Batch 2,250  of  4,384.\n",
            "  Batch 2,300  of  4,384.\n",
            "  Batch 2,350  of  4,384.\n",
            "  Batch 2,400  of  4,384.\n",
            "  Batch 2,450  of  4,384.\n",
            "  Batch 2,500  of  4,384.\n",
            "  Batch 2,550  of  4,384.\n",
            "  Batch 2,600  of  4,384.\n",
            "  Batch 2,650  of  4,384.\n",
            "  Batch 2,700  of  4,384.\n",
            "  Batch 2,750  of  4,384.\n",
            "  Batch 2,800  of  4,384.\n",
            "  Batch 2,850  of  4,384.\n",
            "  Batch 2,900  of  4,384.\n",
            "  Batch 2,950  of  4,384.\n",
            "  Batch 3,000  of  4,384.\n",
            "  Batch 3,050  of  4,384.\n",
            "  Batch 3,100  of  4,384.\n",
            "  Batch 3,150  of  4,384.\n",
            "  Batch 3,200  of  4,384.\n",
            "  Batch 3,250  of  4,384.\n",
            "  Batch 3,300  of  4,384.\n",
            "  Batch 3,350  of  4,384.\n",
            "  Batch 3,400  of  4,384.\n",
            "  Batch 3,450  of  4,384.\n",
            "  Batch 3,500  of  4,384.\n",
            "  Batch 3,550  of  4,384.\n",
            "  Batch 3,600  of  4,384.\n",
            "  Batch 3,650  of  4,384.\n",
            "  Batch 3,700  of  4,384.\n",
            "  Batch 3,750  of  4,384.\n",
            "  Batch 3,800  of  4,384.\n",
            "  Batch 3,850  of  4,384.\n",
            "  Batch 3,900  of  4,384.\n",
            "  Batch 3,950  of  4,384.\n",
            "  Batch 4,000  of  4,384.\n",
            "  Batch 4,050  of  4,384.\n",
            "  Batch 4,100  of  4,384.\n",
            "  Batch 4,150  of  4,384.\n",
            "  Batch 4,200  of  4,384.\n",
            "  Batch 4,250  of  4,384.\n",
            "  Batch 4,300  of  4,384.\n",
            "  Batch 4,350  of  4,384.\n",
            "\n",
            "Training Loss: 0.505\n",
            "Validation Loss: 1.274\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of  1,926.\n",
            "  Batch   100  of  1,926.\n",
            "  Batch   150  of  1,926.\n",
            "  Batch   200  of  1,926.\n",
            "  Batch   250  of  1,926.\n",
            "  Batch   300  of  1,926.\n",
            "  Batch   350  of  1,926.\n",
            "  Batch   400  of  1,926.\n",
            "  Batch   450  of  1,926.\n",
            "  Batch   500  of  1,926.\n",
            "  Batch   550  of  1,926.\n",
            "  Batch   600  of  1,926.\n",
            "  Batch   650  of  1,926.\n",
            "  Batch   700  of  1,926.\n",
            "  Batch   750  of  1,926.\n",
            "  Batch   800  of  1,926.\n",
            "  Batch   850  of  1,926.\n",
            "  Batch   900  of  1,926.\n",
            "  Batch   950  of  1,926.\n",
            "  Batch 1,000  of  1,926.\n",
            "  Batch 1,050  of  1,926.\n",
            "  Batch 1,100  of  1,926.\n",
            "  Batch 1,150  of  1,926.\n",
            "  Batch 1,200  of  1,926.\n",
            "  Batch 1,250  of  1,926.\n",
            "  Batch 1,300  of  1,926.\n",
            "  Batch 1,350  of  1,926.\n",
            "  Batch 1,400  of  1,926.\n",
            "  Batch 1,450  of  1,926.\n",
            "  Batch 1,500  of  1,926.\n",
            "  Batch 1,550  of  1,926.\n",
            "  Batch 1,600  of  1,926.\n",
            "  Batch 1,650  of  1,926.\n",
            "  Batch 1,700  of  1,926.\n",
            "  Batch 1,750  of  1,926.\n",
            "  Batch 1,800  of  1,926.\n",
            "  Batch 1,850  of  1,926.\n",
            "  Batch 1,900  of  1,926.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  4,384.\n",
            "  Batch   100  of  4,384.\n",
            "  Batch   150  of  4,384.\n",
            "  Batch   200  of  4,384.\n",
            "  Batch   250  of  4,384.\n",
            "  Batch   300  of  4,384.\n",
            "  Batch   350  of  4,384.\n",
            "  Batch   400  of  4,384.\n",
            "  Batch   450  of  4,384.\n",
            "  Batch   500  of  4,384.\n",
            "  Batch   550  of  4,384.\n",
            "  Batch   600  of  4,384.\n",
            "  Batch   650  of  4,384.\n",
            "  Batch   700  of  4,384.\n",
            "  Batch   750  of  4,384.\n",
            "  Batch   800  of  4,384.\n",
            "  Batch   850  of  4,384.\n",
            "  Batch   900  of  4,384.\n",
            "  Batch   950  of  4,384.\n",
            "  Batch 1,000  of  4,384.\n",
            "  Batch 1,050  of  4,384.\n",
            "  Batch 1,100  of  4,384.\n",
            "  Batch 1,150  of  4,384.\n",
            "  Batch 1,200  of  4,384.\n",
            "  Batch 1,250  of  4,384.\n",
            "  Batch 1,300  of  4,384.\n",
            "  Batch 1,350  of  4,384.\n",
            "  Batch 1,400  of  4,384.\n",
            "  Batch 1,450  of  4,384.\n",
            "  Batch 1,500  of  4,384.\n",
            "  Batch 1,550  of  4,384.\n",
            "  Batch 1,600  of  4,384.\n",
            "  Batch 1,650  of  4,384.\n",
            "  Batch 1,700  of  4,384.\n",
            "  Batch 1,750  of  4,384.\n",
            "  Batch 1,800  of  4,384.\n",
            "  Batch 1,850  of  4,384.\n",
            "  Batch 1,900  of  4,384.\n",
            "  Batch 1,950  of  4,384.\n",
            "  Batch 2,000  of  4,384.\n",
            "  Batch 2,050  of  4,384.\n",
            "  Batch 2,100  of  4,384.\n",
            "  Batch 2,150  of  4,384.\n",
            "  Batch 2,200  of  4,384.\n",
            "  Batch 2,250  of  4,384.\n",
            "  Batch 2,300  of  4,384.\n",
            "  Batch 2,350  of  4,384.\n",
            "  Batch 2,400  of  4,384.\n",
            "  Batch 2,450  of  4,384.\n",
            "  Batch 2,500  of  4,384.\n",
            "  Batch 2,550  of  4,384.\n",
            "  Batch 2,600  of  4,384.\n",
            "  Batch 2,650  of  4,384.\n",
            "  Batch 2,700  of  4,384.\n",
            "  Batch 2,750  of  4,384.\n",
            "  Batch 2,800  of  4,384.\n",
            "  Batch 2,850  of  4,384.\n",
            "  Batch 2,900  of  4,384.\n",
            "  Batch 2,950  of  4,384.\n",
            "  Batch 3,000  of  4,384.\n",
            "  Batch 3,050  of  4,384.\n",
            "  Batch 3,100  of  4,384.\n",
            "  Batch 3,150  of  4,384.\n",
            "  Batch 3,200  of  4,384.\n",
            "  Batch 3,250  of  4,384.\n",
            "  Batch 3,300  of  4,384.\n",
            "  Batch 3,350  of  4,384.\n",
            "  Batch 3,400  of  4,384.\n",
            "  Batch 3,450  of  4,384.\n",
            "  Batch 3,500  of  4,384.\n",
            "  Batch 3,550  of  4,384.\n",
            "  Batch 3,600  of  4,384.\n",
            "  Batch 3,650  of  4,384.\n",
            "  Batch 3,700  of  4,384.\n",
            "  Batch 3,750  of  4,384.\n",
            "  Batch 3,800  of  4,384.\n",
            "  Batch 3,850  of  4,384.\n",
            "  Batch 3,900  of  4,384.\n",
            "  Batch 3,950  of  4,384.\n",
            "  Batch 4,000  of  4,384.\n",
            "  Batch 4,050  of  4,384.\n",
            "  Batch 4,100  of  4,384.\n",
            "  Batch 4,150  of  4,384.\n",
            "  Batch 4,200  of  4,384.\n",
            "  Batch 4,250  of  4,384.\n",
            "  Batch 4,300  of  4,384.\n",
            "  Batch 4,350  of  4,384.\n",
            "\n",
            "Training Loss: 0.492\n",
            "Validation Loss: 1.034\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of  1,926.\n",
            "  Batch   100  of  1,926.\n",
            "  Batch   150  of  1,926.\n",
            "  Batch   200  of  1,926.\n",
            "  Batch   250  of  1,926.\n",
            "  Batch   300  of  1,926.\n",
            "  Batch   350  of  1,926.\n",
            "  Batch   400  of  1,926.\n",
            "  Batch   450  of  1,926.\n",
            "  Batch   500  of  1,926.\n",
            "  Batch   550  of  1,926.\n",
            "  Batch   600  of  1,926.\n",
            "  Batch   650  of  1,926.\n",
            "  Batch   700  of  1,926.\n",
            "  Batch   750  of  1,926.\n",
            "  Batch   800  of  1,926.\n",
            "  Batch   850  of  1,926.\n",
            "  Batch   900  of  1,926.\n",
            "  Batch   950  of  1,926.\n",
            "  Batch 1,000  of  1,926.\n",
            "  Batch 1,050  of  1,926.\n",
            "  Batch 1,100  of  1,926.\n",
            "  Batch 1,150  of  1,926.\n",
            "  Batch 1,200  of  1,926.\n",
            "  Batch 1,250  of  1,926.\n",
            "  Batch 1,300  of  1,926.\n",
            "  Batch 1,350  of  1,926.\n",
            "  Batch 1,400  of  1,926.\n",
            "  Batch 1,450  of  1,926.\n",
            "  Batch 1,500  of  1,926.\n",
            "  Batch 1,550  of  1,926.\n",
            "  Batch 1,600  of  1,926.\n",
            "  Batch 1,650  of  1,926.\n",
            "  Batch 1,700  of  1,926.\n",
            "  Batch 1,750  of  1,926.\n",
            "  Batch 1,800  of  1,926.\n",
            "  Batch 1,850  of  1,926.\n",
            "  Batch 1,900  of  1,926.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  4,384.\n",
            "  Batch   100  of  4,384.\n",
            "  Batch   150  of  4,384.\n",
            "  Batch   200  of  4,384.\n",
            "  Batch   250  of  4,384.\n",
            "  Batch   300  of  4,384.\n",
            "  Batch   350  of  4,384.\n",
            "  Batch   400  of  4,384.\n",
            "  Batch   450  of  4,384.\n",
            "  Batch   500  of  4,384.\n",
            "  Batch   550  of  4,384.\n",
            "  Batch   600  of  4,384.\n",
            "  Batch   650  of  4,384.\n",
            "  Batch   700  of  4,384.\n",
            "  Batch   750  of  4,384.\n",
            "  Batch   800  of  4,384.\n",
            "  Batch   850  of  4,384.\n",
            "  Batch   900  of  4,384.\n",
            "  Batch   950  of  4,384.\n",
            "  Batch 1,000  of  4,384.\n",
            "  Batch 1,050  of  4,384.\n",
            "  Batch 1,100  of  4,384.\n",
            "  Batch 1,150  of  4,384.\n",
            "  Batch 1,200  of  4,384.\n",
            "  Batch 1,250  of  4,384.\n",
            "  Batch 1,300  of  4,384.\n",
            "  Batch 1,350  of  4,384.\n",
            "  Batch 1,400  of  4,384.\n",
            "  Batch 1,450  of  4,384.\n",
            "  Batch 1,500  of  4,384.\n",
            "  Batch 1,550  of  4,384.\n",
            "  Batch 1,600  of  4,384.\n",
            "  Batch 1,650  of  4,384.\n",
            "  Batch 1,700  of  4,384.\n",
            "  Batch 1,750  of  4,384.\n",
            "  Batch 1,800  of  4,384.\n",
            "  Batch 1,850  of  4,384.\n",
            "  Batch 1,900  of  4,384.\n",
            "  Batch 1,950  of  4,384.\n",
            "  Batch 2,000  of  4,384.\n",
            "  Batch 2,050  of  4,384.\n",
            "  Batch 2,100  of  4,384.\n",
            "  Batch 2,150  of  4,384.\n",
            "  Batch 2,200  of  4,384.\n",
            "  Batch 2,250  of  4,384.\n",
            "  Batch 2,300  of  4,384.\n",
            "  Batch 2,350  of  4,384.\n",
            "  Batch 2,400  of  4,384.\n",
            "  Batch 2,450  of  4,384.\n",
            "  Batch 2,500  of  4,384.\n",
            "  Batch 2,550  of  4,384.\n",
            "  Batch 2,600  of  4,384.\n",
            "  Batch 2,650  of  4,384.\n",
            "  Batch 2,700  of  4,384.\n",
            "  Batch 2,750  of  4,384.\n",
            "  Batch 2,800  of  4,384.\n",
            "  Batch 2,850  of  4,384.\n",
            "  Batch 2,900  of  4,384.\n",
            "  Batch 2,950  of  4,384.\n",
            "  Batch 3,000  of  4,384.\n",
            "  Batch 3,050  of  4,384.\n",
            "  Batch 3,100  of  4,384.\n",
            "  Batch 3,150  of  4,384.\n",
            "  Batch 3,200  of  4,384.\n",
            "  Batch 3,250  of  4,384.\n",
            "  Batch 3,300  of  4,384.\n",
            "  Batch 3,350  of  4,384.\n",
            "  Batch 3,400  of  4,384.\n",
            "  Batch 3,450  of  4,384.\n",
            "  Batch 3,500  of  4,384.\n",
            "  Batch 3,550  of  4,384.\n",
            "  Batch 3,600  of  4,384.\n",
            "  Batch 3,650  of  4,384.\n",
            "  Batch 3,700  of  4,384.\n",
            "  Batch 3,750  of  4,384.\n",
            "  Batch 3,800  of  4,384.\n",
            "  Batch 3,850  of  4,384.\n",
            "  Batch 3,900  of  4,384.\n",
            "  Batch 3,950  of  4,384.\n",
            "  Batch 4,000  of  4,384.\n",
            "  Batch 4,050  of  4,384.\n",
            "  Batch 4,100  of  4,384.\n",
            "  Batch 4,150  of  4,384.\n",
            "  Batch 4,200  of  4,384.\n",
            "  Batch 4,250  of  4,384.\n",
            "  Batch 4,300  of  4,384.\n",
            "  Batch 4,350  of  4,384.\n",
            "\n",
            "Training Loss: 0.480\n",
            "Validation Loss: 1.232\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of  1,926.\n",
            "  Batch   100  of  1,926.\n",
            "  Batch   150  of  1,926.\n",
            "  Batch   200  of  1,926.\n",
            "  Batch   250  of  1,926.\n",
            "  Batch   300  of  1,926.\n",
            "  Batch   350  of  1,926.\n",
            "  Batch   400  of  1,926.\n",
            "  Batch   450  of  1,926.\n",
            "  Batch   500  of  1,926.\n",
            "  Batch   550  of  1,926.\n",
            "  Batch   600  of  1,926.\n",
            "  Batch   650  of  1,926.\n",
            "  Batch   700  of  1,926.\n",
            "  Batch   750  of  1,926.\n",
            "  Batch   800  of  1,926.\n",
            "  Batch   850  of  1,926.\n",
            "  Batch   900  of  1,926.\n",
            "  Batch   950  of  1,926.\n",
            "  Batch 1,000  of  1,926.\n",
            "  Batch 1,050  of  1,926.\n",
            "  Batch 1,100  of  1,926.\n",
            "  Batch 1,150  of  1,926.\n",
            "  Batch 1,200  of  1,926.\n",
            "  Batch 1,250  of  1,926.\n",
            "  Batch 1,300  of  1,926.\n",
            "  Batch 1,350  of  1,926.\n",
            "  Batch 1,400  of  1,926.\n",
            "  Batch 1,450  of  1,926.\n",
            "  Batch 1,500  of  1,926.\n",
            "  Batch 1,550  of  1,926.\n",
            "  Batch 1,600  of  1,926.\n",
            "  Batch 1,650  of  1,926.\n",
            "  Batch 1,700  of  1,926.\n",
            "  Batch 1,750  of  1,926.\n",
            "  Batch 1,800  of  1,926.\n",
            "  Batch 1,850  of  1,926.\n",
            "  Batch 1,900  of  1,926.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  4,384.\n",
            "  Batch   100  of  4,384.\n",
            "  Batch   150  of  4,384.\n",
            "  Batch   200  of  4,384.\n",
            "  Batch   250  of  4,384.\n",
            "  Batch   300  of  4,384.\n",
            "  Batch   350  of  4,384.\n",
            "  Batch   400  of  4,384.\n",
            "  Batch   450  of  4,384.\n",
            "  Batch   500  of  4,384.\n",
            "  Batch   550  of  4,384.\n",
            "  Batch   600  of  4,384.\n",
            "  Batch   650  of  4,384.\n",
            "  Batch   700  of  4,384.\n",
            "  Batch   750  of  4,384.\n",
            "  Batch   800  of  4,384.\n",
            "  Batch   850  of  4,384.\n",
            "  Batch   900  of  4,384.\n",
            "  Batch   950  of  4,384.\n",
            "  Batch 1,000  of  4,384.\n",
            "  Batch 1,050  of  4,384.\n",
            "  Batch 1,100  of  4,384.\n",
            "  Batch 1,150  of  4,384.\n",
            "  Batch 1,200  of  4,384.\n",
            "  Batch 1,250  of  4,384.\n",
            "  Batch 1,300  of  4,384.\n",
            "  Batch 1,350  of  4,384.\n",
            "  Batch 1,400  of  4,384.\n",
            "  Batch 1,450  of  4,384.\n",
            "  Batch 1,500  of  4,384.\n",
            "  Batch 1,550  of  4,384.\n",
            "  Batch 1,600  of  4,384.\n",
            "  Batch 1,650  of  4,384.\n",
            "  Batch 1,700  of  4,384.\n",
            "  Batch 1,750  of  4,384.\n",
            "  Batch 1,800  of  4,384.\n",
            "  Batch 1,850  of  4,384.\n",
            "  Batch 1,900  of  4,384.\n",
            "  Batch 1,950  of  4,384.\n",
            "  Batch 2,000  of  4,384.\n",
            "  Batch 2,050  of  4,384.\n",
            "  Batch 2,100  of  4,384.\n",
            "  Batch 2,150  of  4,384.\n",
            "  Batch 2,200  of  4,384.\n",
            "  Batch 2,250  of  4,384.\n",
            "  Batch 2,300  of  4,384.\n",
            "  Batch 2,350  of  4,384.\n",
            "  Batch 2,400  of  4,384.\n",
            "  Batch 2,450  of  4,384.\n",
            "  Batch 2,500  of  4,384.\n",
            "  Batch 2,550  of  4,384.\n",
            "  Batch 2,600  of  4,384.\n",
            "  Batch 2,650  of  4,384.\n",
            "  Batch 2,700  of  4,384.\n",
            "  Batch 2,750  of  4,384.\n",
            "  Batch 2,800  of  4,384.\n",
            "  Batch 2,850  of  4,384.\n",
            "  Batch 2,900  of  4,384.\n",
            "  Batch 2,950  of  4,384.\n",
            "  Batch 3,000  of  4,384.\n",
            "  Batch 3,050  of  4,384.\n",
            "  Batch 3,100  of  4,384.\n",
            "  Batch 3,150  of  4,384.\n",
            "  Batch 3,200  of  4,384.\n",
            "  Batch 3,250  of  4,384.\n",
            "  Batch 3,300  of  4,384.\n",
            "  Batch 3,350  of  4,384.\n",
            "  Batch 3,400  of  4,384.\n",
            "  Batch 3,450  of  4,384.\n",
            "  Batch 3,500  of  4,384.\n",
            "  Batch 3,550  of  4,384.\n",
            "  Batch 3,600  of  4,384.\n",
            "  Batch 3,650  of  4,384.\n",
            "  Batch 3,700  of  4,384.\n",
            "  Batch 3,750  of  4,384.\n",
            "  Batch 3,800  of  4,384.\n",
            "  Batch 3,850  of  4,384.\n",
            "  Batch 3,900  of  4,384.\n",
            "  Batch 3,950  of  4,384.\n",
            "  Batch 4,000  of  4,384.\n",
            "  Batch 4,050  of  4,384.\n",
            "  Batch 4,100  of  4,384.\n",
            "  Batch 4,150  of  4,384.\n",
            "  Batch 4,200  of  4,384.\n",
            "  Batch 4,250  of  4,384.\n",
            "  Batch 4,300  of  4,384.\n",
            "  Batch 4,350  of  4,384.\n",
            "\n",
            "Training Loss: 0.480\n",
            "Validation Loss: 0.871\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of  1,926.\n",
            "  Batch   100  of  1,926.\n",
            "  Batch   150  of  1,926.\n",
            "  Batch   200  of  1,926.\n",
            "  Batch   250  of  1,926.\n",
            "  Batch   300  of  1,926.\n",
            "  Batch   350  of  1,926.\n",
            "  Batch   400  of  1,926.\n",
            "  Batch   450  of  1,926.\n",
            "  Batch   500  of  1,926.\n",
            "  Batch   550  of  1,926.\n",
            "  Batch   600  of  1,926.\n",
            "  Batch   650  of  1,926.\n",
            "  Batch   700  of  1,926.\n",
            "  Batch   750  of  1,926.\n",
            "  Batch   800  of  1,926.\n",
            "  Batch   850  of  1,926.\n",
            "  Batch   900  of  1,926.\n",
            "  Batch   950  of  1,926.\n",
            "  Batch 1,000  of  1,926.\n",
            "  Batch 1,050  of  1,926.\n",
            "  Batch 1,100  of  1,926.\n",
            "  Batch 1,150  of  1,926.\n",
            "  Batch 1,200  of  1,926.\n",
            "  Batch 1,250  of  1,926.\n",
            "  Batch 1,300  of  1,926.\n",
            "  Batch 1,350  of  1,926.\n",
            "  Batch 1,400  of  1,926.\n",
            "  Batch 1,450  of  1,926.\n",
            "  Batch 1,500  of  1,926.\n",
            "  Batch 1,550  of  1,926.\n",
            "  Batch 1,600  of  1,926.\n",
            "  Batch 1,650  of  1,926.\n",
            "  Batch 1,700  of  1,926.\n",
            "  Batch 1,750  of  1,926.\n",
            "  Batch 1,800  of  1,926.\n",
            "  Batch 1,850  of  1,926.\n",
            "  Batch 1,900  of  1,926.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  4,384.\n",
            "  Batch   100  of  4,384.\n",
            "  Batch   150  of  4,384.\n",
            "  Batch   200  of  4,384.\n",
            "  Batch   250  of  4,384.\n",
            "  Batch   300  of  4,384.\n",
            "  Batch   350  of  4,384.\n",
            "  Batch   400  of  4,384.\n",
            "  Batch   450  of  4,384.\n",
            "  Batch   500  of  4,384.\n",
            "  Batch   550  of  4,384.\n",
            "  Batch   600  of  4,384.\n",
            "  Batch   650  of  4,384.\n",
            "  Batch   700  of  4,384.\n",
            "  Batch   750  of  4,384.\n",
            "  Batch   800  of  4,384.\n",
            "  Batch   850  of  4,384.\n",
            "  Batch   900  of  4,384.\n",
            "  Batch   950  of  4,384.\n",
            "  Batch 1,000  of  4,384.\n",
            "  Batch 1,050  of  4,384.\n",
            "  Batch 1,100  of  4,384.\n",
            "  Batch 1,150  of  4,384.\n",
            "  Batch 1,200  of  4,384.\n",
            "  Batch 1,250  of  4,384.\n",
            "  Batch 1,300  of  4,384.\n",
            "  Batch 1,350  of  4,384.\n",
            "  Batch 1,400  of  4,384.\n",
            "  Batch 1,450  of  4,384.\n",
            "  Batch 1,500  of  4,384.\n",
            "  Batch 1,550  of  4,384.\n",
            "  Batch 1,600  of  4,384.\n",
            "  Batch 1,650  of  4,384.\n",
            "  Batch 1,700  of  4,384.\n",
            "  Batch 1,750  of  4,384.\n",
            "  Batch 1,800  of  4,384.\n",
            "  Batch 1,850  of  4,384.\n",
            "  Batch 1,900  of  4,384.\n",
            "  Batch 1,950  of  4,384.\n",
            "  Batch 2,000  of  4,384.\n",
            "  Batch 2,050  of  4,384.\n",
            "  Batch 2,100  of  4,384.\n",
            "  Batch 2,150  of  4,384.\n",
            "  Batch 2,200  of  4,384.\n",
            "  Batch 2,250  of  4,384.\n",
            "  Batch 2,300  of  4,384.\n",
            "  Batch 2,350  of  4,384.\n",
            "  Batch 2,400  of  4,384.\n",
            "  Batch 2,450  of  4,384.\n",
            "  Batch 2,500  of  4,384.\n",
            "  Batch 2,550  of  4,384.\n",
            "  Batch 2,600  of  4,384.\n",
            "  Batch 2,650  of  4,384.\n",
            "  Batch 2,700  of  4,384.\n",
            "  Batch 2,750  of  4,384.\n",
            "  Batch 2,800  of  4,384.\n",
            "  Batch 2,850  of  4,384.\n",
            "  Batch 2,900  of  4,384.\n",
            "  Batch 2,950  of  4,384.\n",
            "  Batch 3,000  of  4,384.\n",
            "  Batch 3,050  of  4,384.\n",
            "  Batch 3,100  of  4,384.\n",
            "  Batch 3,150  of  4,384.\n",
            "  Batch 3,200  of  4,384.\n",
            "  Batch 3,250  of  4,384.\n",
            "  Batch 3,300  of  4,384.\n",
            "  Batch 3,350  of  4,384.\n",
            "  Batch 3,400  of  4,384.\n",
            "  Batch 3,450  of  4,384.\n",
            "  Batch 3,500  of  4,384.\n",
            "  Batch 3,550  of  4,384.\n",
            "  Batch 3,600  of  4,384.\n",
            "  Batch 3,650  of  4,384.\n",
            "  Batch 3,700  of  4,384.\n",
            "  Batch 3,750  of  4,384.\n",
            "  Batch 3,800  of  4,384.\n",
            "  Batch 3,850  of  4,384.\n",
            "  Batch 3,900  of  4,384.\n",
            "  Batch 3,950  of  4,384.\n",
            "  Batch 4,000  of  4,384.\n",
            "  Batch 4,050  of  4,384.\n",
            "  Batch 4,100  of  4,384.\n",
            "  Batch 4,150  of  4,384.\n",
            "  Batch 4,200  of  4,384.\n",
            "  Batch 4,250  of  4,384.\n",
            "  Batch 4,300  of  4,384.\n",
            "  Batch 4,350  of  4,384.\n",
            "\n",
            "Training Loss: 0.467\n",
            "Validation Loss: 0.817\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of  1,926.\n",
            "  Batch   100  of  1,926.\n",
            "  Batch   150  of  1,926.\n",
            "  Batch   200  of  1,926.\n",
            "  Batch   250  of  1,926.\n",
            "  Batch   300  of  1,926.\n",
            "  Batch   350  of  1,926.\n",
            "  Batch   400  of  1,926.\n",
            "  Batch   450  of  1,926.\n",
            "  Batch   500  of  1,926.\n",
            "  Batch   550  of  1,926.\n",
            "  Batch   600  of  1,926.\n",
            "  Batch   650  of  1,926.\n",
            "  Batch   700  of  1,926.\n",
            "  Batch   750  of  1,926.\n",
            "  Batch   800  of  1,926.\n",
            "  Batch   850  of  1,926.\n",
            "  Batch   900  of  1,926.\n",
            "  Batch   950  of  1,926.\n",
            "  Batch 1,000  of  1,926.\n",
            "  Batch 1,050  of  1,926.\n",
            "  Batch 1,100  of  1,926.\n",
            "  Batch 1,150  of  1,926.\n",
            "  Batch 1,200  of  1,926.\n",
            "  Batch 1,250  of  1,926.\n",
            "  Batch 1,300  of  1,926.\n",
            "  Batch 1,350  of  1,926.\n",
            "  Batch 1,400  of  1,926.\n",
            "  Batch 1,450  of  1,926.\n",
            "  Batch 1,500  of  1,926.\n",
            "  Batch 1,550  of  1,926.\n",
            "  Batch 1,600  of  1,926.\n",
            "  Batch 1,650  of  1,926.\n",
            "  Batch 1,700  of  1,926.\n",
            "  Batch 1,750  of  1,926.\n",
            "  Batch 1,800  of  1,926.\n",
            "  Batch 1,850  of  1,926.\n",
            "  Batch 1,900  of  1,926.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  4,384.\n",
            "  Batch   100  of  4,384.\n",
            "  Batch   150  of  4,384.\n",
            "  Batch   200  of  4,384.\n",
            "  Batch   250  of  4,384.\n",
            "  Batch   300  of  4,384.\n",
            "  Batch   350  of  4,384.\n",
            "  Batch   400  of  4,384.\n",
            "  Batch   450  of  4,384.\n",
            "  Batch   500  of  4,384.\n",
            "  Batch   550  of  4,384.\n",
            "  Batch   600  of  4,384.\n",
            "  Batch   650  of  4,384.\n",
            "  Batch   700  of  4,384.\n",
            "  Batch   750  of  4,384.\n",
            "  Batch   800  of  4,384.\n",
            "  Batch   850  of  4,384.\n",
            "  Batch   900  of  4,384.\n",
            "  Batch   950  of  4,384.\n",
            "  Batch 1,000  of  4,384.\n",
            "  Batch 1,050  of  4,384.\n",
            "  Batch 1,100  of  4,384.\n",
            "  Batch 1,150  of  4,384.\n",
            "  Batch 1,200  of  4,384.\n",
            "  Batch 1,250  of  4,384.\n",
            "  Batch 1,300  of  4,384.\n",
            "  Batch 1,350  of  4,384.\n",
            "  Batch 1,400  of  4,384.\n",
            "  Batch 1,450  of  4,384.\n",
            "  Batch 1,500  of  4,384.\n",
            "  Batch 1,550  of  4,384.\n",
            "  Batch 1,600  of  4,384.\n",
            "  Batch 1,650  of  4,384.\n",
            "  Batch 1,700  of  4,384.\n",
            "  Batch 1,750  of  4,384.\n",
            "  Batch 1,800  of  4,384.\n",
            "  Batch 1,850  of  4,384.\n",
            "  Batch 1,900  of  4,384.\n",
            "  Batch 1,950  of  4,384.\n",
            "  Batch 2,000  of  4,384.\n",
            "  Batch 2,050  of  4,384.\n",
            "  Batch 2,100  of  4,384.\n",
            "  Batch 2,150  of  4,384.\n",
            "  Batch 2,200  of  4,384.\n",
            "  Batch 2,250  of  4,384.\n",
            "  Batch 2,300  of  4,384.\n",
            "  Batch 2,350  of  4,384.\n",
            "  Batch 2,400  of  4,384.\n",
            "  Batch 2,450  of  4,384.\n",
            "  Batch 2,500  of  4,384.\n",
            "  Batch 2,550  of  4,384.\n",
            "  Batch 2,600  of  4,384.\n",
            "  Batch 2,650  of  4,384.\n",
            "  Batch 2,700  of  4,384.\n",
            "  Batch 2,750  of  4,384.\n",
            "  Batch 2,800  of  4,384.\n",
            "  Batch 2,850  of  4,384.\n",
            "  Batch 2,900  of  4,384.\n",
            "  Batch 2,950  of  4,384.\n",
            "  Batch 3,000  of  4,384.\n",
            "  Batch 3,050  of  4,384.\n",
            "  Batch 3,100  of  4,384.\n",
            "  Batch 3,150  of  4,384.\n",
            "  Batch 3,200  of  4,384.\n",
            "  Batch 3,250  of  4,384.\n",
            "  Batch 3,300  of  4,384.\n",
            "  Batch 3,350  of  4,384.\n",
            "  Batch 3,400  of  4,384.\n",
            "  Batch 3,450  of  4,384.\n",
            "  Batch 3,500  of  4,384.\n",
            "  Batch 3,550  of  4,384.\n",
            "  Batch 3,600  of  4,384.\n",
            "  Batch 3,650  of  4,384.\n",
            "  Batch 3,700  of  4,384.\n",
            "  Batch 3,750  of  4,384.\n",
            "  Batch 3,800  of  4,384.\n",
            "  Batch 3,850  of  4,384.\n",
            "  Batch 3,900  of  4,384.\n",
            "  Batch 3,950  of  4,384.\n",
            "  Batch 4,000  of  4,384.\n",
            "  Batch 4,050  of  4,384.\n",
            "  Batch 4,100  of  4,384.\n",
            "  Batch 4,150  of  4,384.\n",
            "  Batch 4,200  of  4,384.\n",
            "  Batch 4,250  of  4,384.\n",
            "  Batch 4,300  of  4,384.\n",
            "  Batch 4,350  of  4,384.\n",
            "\n",
            "Training Loss: 0.470\n",
            "Validation Loss: 1.109\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of  1,926.\n",
            "  Batch   100  of  1,926.\n",
            "  Batch   150  of  1,926.\n",
            "  Batch   200  of  1,926.\n",
            "  Batch   250  of  1,926.\n",
            "  Batch   300  of  1,926.\n",
            "  Batch   350  of  1,926.\n",
            "  Batch   400  of  1,926.\n",
            "  Batch   450  of  1,926.\n",
            "  Batch   500  of  1,926.\n",
            "  Batch   550  of  1,926.\n",
            "  Batch   600  of  1,926.\n",
            "  Batch   650  of  1,926.\n",
            "  Batch   700  of  1,926.\n",
            "  Batch   750  of  1,926.\n",
            "  Batch   800  of  1,926.\n",
            "  Batch   850  of  1,926.\n",
            "  Batch   900  of  1,926.\n",
            "  Batch   950  of  1,926.\n",
            "  Batch 1,000  of  1,926.\n",
            "  Batch 1,050  of  1,926.\n",
            "  Batch 1,100  of  1,926.\n",
            "  Batch 1,150  of  1,926.\n",
            "  Batch 1,200  of  1,926.\n",
            "  Batch 1,250  of  1,926.\n",
            "  Batch 1,300  of  1,926.\n",
            "  Batch 1,350  of  1,926.\n",
            "  Batch 1,400  of  1,926.\n",
            "  Batch 1,450  of  1,926.\n",
            "  Batch 1,500  of  1,926.\n",
            "  Batch 1,550  of  1,926.\n",
            "  Batch 1,600  of  1,926.\n",
            "  Batch 1,650  of  1,926.\n",
            "  Batch 1,700  of  1,926.\n",
            "  Batch 1,750  of  1,926.\n",
            "  Batch 1,800  of  1,926.\n",
            "  Batch 1,850  of  1,926.\n",
            "  Batch 1,900  of  1,926.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  4,384.\n",
            "  Batch   100  of  4,384.\n",
            "  Batch   150  of  4,384.\n",
            "  Batch   200  of  4,384.\n",
            "  Batch   250  of  4,384.\n",
            "  Batch   300  of  4,384.\n",
            "  Batch   350  of  4,384.\n",
            "  Batch   400  of  4,384.\n",
            "  Batch   450  of  4,384.\n",
            "  Batch   500  of  4,384.\n",
            "  Batch   550  of  4,384.\n",
            "  Batch   600  of  4,384.\n",
            "  Batch   650  of  4,384.\n",
            "  Batch   700  of  4,384.\n",
            "  Batch   750  of  4,384.\n",
            "  Batch   800  of  4,384.\n",
            "  Batch   850  of  4,384.\n",
            "  Batch   900  of  4,384.\n",
            "  Batch   950  of  4,384.\n",
            "  Batch 1,000  of  4,384.\n",
            "  Batch 1,050  of  4,384.\n",
            "  Batch 1,100  of  4,384.\n",
            "  Batch 1,150  of  4,384.\n",
            "  Batch 1,200  of  4,384.\n",
            "  Batch 1,250  of  4,384.\n",
            "  Batch 1,300  of  4,384.\n",
            "  Batch 1,350  of  4,384.\n",
            "  Batch 1,400  of  4,384.\n",
            "  Batch 1,450  of  4,384.\n",
            "  Batch 1,500  of  4,384.\n",
            "  Batch 1,550  of  4,384.\n",
            "  Batch 1,600  of  4,384.\n",
            "  Batch 1,650  of  4,384.\n",
            "  Batch 1,700  of  4,384.\n",
            "  Batch 1,750  of  4,384.\n",
            "  Batch 1,800  of  4,384.\n",
            "  Batch 1,850  of  4,384.\n",
            "  Batch 1,900  of  4,384.\n",
            "  Batch 1,950  of  4,384.\n",
            "  Batch 2,000  of  4,384.\n",
            "  Batch 2,050  of  4,384.\n",
            "  Batch 2,100  of  4,384.\n",
            "  Batch 2,150  of  4,384.\n",
            "  Batch 2,200  of  4,384.\n",
            "  Batch 2,250  of  4,384.\n",
            "  Batch 2,300  of  4,384.\n",
            "  Batch 2,350  of  4,384.\n",
            "  Batch 2,400  of  4,384.\n",
            "  Batch 2,450  of  4,384.\n",
            "  Batch 2,500  of  4,384.\n",
            "  Batch 2,550  of  4,384.\n",
            "  Batch 2,600  of  4,384.\n",
            "  Batch 2,650  of  4,384.\n",
            "  Batch 2,700  of  4,384.\n",
            "  Batch 2,750  of  4,384.\n",
            "  Batch 2,800  of  4,384.\n",
            "  Batch 2,850  of  4,384.\n",
            "  Batch 2,900  of  4,384.\n",
            "  Batch 2,950  of  4,384.\n",
            "  Batch 3,000  of  4,384.\n",
            "  Batch 3,050  of  4,384.\n",
            "  Batch 3,100  of  4,384.\n",
            "  Batch 3,150  of  4,384.\n",
            "  Batch 3,200  of  4,384.\n",
            "  Batch 3,250  of  4,384.\n",
            "  Batch 3,300  of  4,384.\n",
            "  Batch 3,350  of  4,384.\n",
            "  Batch 3,400  of  4,384.\n",
            "  Batch 3,450  of  4,384.\n",
            "  Batch 3,500  of  4,384.\n",
            "  Batch 3,550  of  4,384.\n",
            "  Batch 3,600  of  4,384.\n",
            "  Batch 3,650  of  4,384.\n",
            "  Batch 3,700  of  4,384.\n",
            "  Batch 3,750  of  4,384.\n",
            "  Batch 3,800  of  4,384.\n",
            "  Batch 3,850  of  4,384.\n",
            "  Batch 3,900  of  4,384.\n",
            "  Batch 3,950  of  4,384.\n",
            "  Batch 4,000  of  4,384.\n",
            "  Batch 4,050  of  4,384.\n",
            "  Batch 4,100  of  4,384.\n",
            "  Batch 4,150  of  4,384.\n",
            "  Batch 4,200  of  4,384.\n",
            "  Batch 4,250  of  4,384.\n",
            "  Batch 4,300  of  4,384.\n",
            "  Batch 4,350  of  4,384.\n",
            "\n",
            "Training Loss: 0.465\n",
            "Validation Loss: 1.085\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of  1,926.\n",
            "  Batch   100  of  1,926.\n",
            "  Batch   150  of  1,926.\n",
            "  Batch   200  of  1,926.\n",
            "  Batch   250  of  1,926.\n",
            "  Batch   300  of  1,926.\n",
            "  Batch   350  of  1,926.\n",
            "  Batch   400  of  1,926.\n",
            "  Batch   450  of  1,926.\n",
            "  Batch   500  of  1,926.\n",
            "  Batch   550  of  1,926.\n",
            "  Batch   600  of  1,926.\n",
            "  Batch   650  of  1,926.\n",
            "  Batch   700  of  1,926.\n",
            "  Batch   750  of  1,926.\n",
            "  Batch   800  of  1,926.\n",
            "  Batch   850  of  1,926.\n",
            "  Batch   900  of  1,926.\n",
            "  Batch   950  of  1,926.\n",
            "  Batch 1,000  of  1,926.\n",
            "  Batch 1,050  of  1,926.\n",
            "  Batch 1,100  of  1,926.\n",
            "  Batch 1,150  of  1,926.\n",
            "  Batch 1,200  of  1,926.\n",
            "  Batch 1,250  of  1,926.\n",
            "  Batch 1,300  of  1,926.\n",
            "  Batch 1,350  of  1,926.\n",
            "  Batch 1,400  of  1,926.\n",
            "  Batch 1,450  of  1,926.\n",
            "  Batch 1,500  of  1,926.\n",
            "  Batch 1,550  of  1,926.\n",
            "  Batch 1,600  of  1,926.\n",
            "  Batch 1,650  of  1,926.\n",
            "  Batch 1,700  of  1,926.\n",
            "  Batch 1,750  of  1,926.\n",
            "  Batch 1,800  of  1,926.\n",
            "  Batch 1,850  of  1,926.\n",
            "  Batch 1,900  of  1,926.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  4,384.\n",
            "  Batch   100  of  4,384.\n",
            "  Batch   150  of  4,384.\n",
            "  Batch   200  of  4,384.\n",
            "  Batch   250  of  4,384.\n",
            "  Batch   300  of  4,384.\n",
            "  Batch   350  of  4,384.\n",
            "  Batch   400  of  4,384.\n",
            "  Batch   450  of  4,384.\n",
            "  Batch   500  of  4,384.\n",
            "  Batch   550  of  4,384.\n",
            "  Batch   600  of  4,384.\n",
            "  Batch   650  of  4,384.\n",
            "  Batch   700  of  4,384.\n",
            "  Batch   750  of  4,384.\n",
            "  Batch   800  of  4,384.\n",
            "  Batch   850  of  4,384.\n",
            "  Batch   900  of  4,384.\n",
            "  Batch   950  of  4,384.\n",
            "  Batch 1,000  of  4,384.\n",
            "  Batch 1,050  of  4,384.\n",
            "  Batch 1,100  of  4,384.\n",
            "  Batch 1,150  of  4,384.\n",
            "  Batch 1,200  of  4,384.\n",
            "  Batch 1,250  of  4,384.\n",
            "  Batch 1,300  of  4,384.\n",
            "  Batch 1,350  of  4,384.\n",
            "  Batch 1,400  of  4,384.\n",
            "  Batch 1,450  of  4,384.\n",
            "  Batch 1,500  of  4,384.\n",
            "  Batch 1,550  of  4,384.\n",
            "  Batch 1,600  of  4,384.\n",
            "  Batch 1,650  of  4,384.\n",
            "  Batch 1,700  of  4,384.\n",
            "  Batch 1,750  of  4,384.\n",
            "  Batch 1,800  of  4,384.\n",
            "  Batch 1,850  of  4,384.\n",
            "  Batch 1,900  of  4,384.\n",
            "  Batch 1,950  of  4,384.\n",
            "  Batch 2,000  of  4,384.\n",
            "  Batch 2,050  of  4,384.\n",
            "  Batch 2,100  of  4,384.\n",
            "  Batch 2,150  of  4,384.\n",
            "  Batch 2,200  of  4,384.\n",
            "  Batch 2,250  of  4,384.\n",
            "  Batch 2,300  of  4,384.\n",
            "  Batch 2,350  of  4,384.\n",
            "  Batch 2,400  of  4,384.\n",
            "  Batch 2,450  of  4,384.\n",
            "  Batch 2,500  of  4,384.\n",
            "  Batch 2,550  of  4,384.\n",
            "  Batch 2,600  of  4,384.\n",
            "  Batch 2,650  of  4,384.\n",
            "  Batch 2,700  of  4,384.\n",
            "  Batch 2,750  of  4,384.\n",
            "  Batch 2,800  of  4,384.\n",
            "  Batch 2,850  of  4,384.\n",
            "  Batch 2,900  of  4,384.\n",
            "  Batch 2,950  of  4,384.\n",
            "  Batch 3,000  of  4,384.\n",
            "  Batch 3,050  of  4,384.\n",
            "  Batch 3,100  of  4,384.\n",
            "  Batch 3,150  of  4,384.\n",
            "  Batch 3,200  of  4,384.\n",
            "  Batch 3,250  of  4,384.\n",
            "  Batch 3,300  of  4,384.\n",
            "  Batch 3,350  of  4,384.\n",
            "  Batch 3,400  of  4,384.\n",
            "  Batch 3,450  of  4,384.\n",
            "  Batch 3,500  of  4,384.\n",
            "  Batch 3,550  of  4,384.\n",
            "  Batch 3,600  of  4,384.\n",
            "  Batch 3,650  of  4,384.\n",
            "  Batch 3,700  of  4,384.\n",
            "  Batch 3,750  of  4,384.\n",
            "  Batch 3,800  of  4,384.\n",
            "  Batch 3,850  of  4,384.\n",
            "  Batch 3,900  of  4,384.\n",
            "  Batch 3,950  of  4,384.\n",
            "  Batch 4,000  of  4,384.\n",
            "  Batch 4,050  of  4,384.\n",
            "  Batch 4,100  of  4,384.\n",
            "  Batch 4,150  of  4,384.\n",
            "  Batch 4,200  of  4,384.\n",
            "  Batch 4,250  of  4,384.\n",
            "  Batch 4,300  of  4,384.\n",
            "  Batch 4,350  of  4,384.\n",
            "\n",
            "Training Loss: 0.463\n",
            "Validation Loss: 0.923\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of  1,926.\n",
            "  Batch   100  of  1,926.\n",
            "  Batch   150  of  1,926.\n",
            "  Batch   200  of  1,926.\n",
            "  Batch   250  of  1,926.\n",
            "  Batch   300  of  1,926.\n",
            "  Batch   350  of  1,926.\n",
            "  Batch   400  of  1,926.\n",
            "  Batch   450  of  1,926.\n",
            "  Batch   500  of  1,926.\n",
            "  Batch   550  of  1,926.\n",
            "  Batch   600  of  1,926.\n",
            "  Batch   650  of  1,926.\n",
            "  Batch   700  of  1,926.\n",
            "  Batch   750  of  1,926.\n",
            "  Batch   800  of  1,926.\n",
            "  Batch   850  of  1,926.\n",
            "  Batch   900  of  1,926.\n",
            "  Batch   950  of  1,926.\n",
            "  Batch 1,000  of  1,926.\n",
            "  Batch 1,050  of  1,926.\n",
            "  Batch 1,100  of  1,926.\n",
            "  Batch 1,150  of  1,926.\n",
            "  Batch 1,200  of  1,926.\n",
            "  Batch 1,250  of  1,926.\n",
            "  Batch 1,300  of  1,926.\n",
            "  Batch 1,350  of  1,926.\n",
            "  Batch 1,400  of  1,926.\n",
            "  Batch 1,450  of  1,926.\n",
            "  Batch 1,500  of  1,926.\n",
            "  Batch 1,550  of  1,926.\n",
            "  Batch 1,600  of  1,926.\n",
            "  Batch 1,650  of  1,926.\n",
            "  Batch 1,700  of  1,926.\n",
            "  Batch 1,750  of  1,926.\n",
            "  Batch 1,800  of  1,926.\n",
            "  Batch 1,850  of  1,926.\n",
            "  Batch 1,900  of  1,926.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  4,384.\n",
            "  Batch   100  of  4,384.\n",
            "  Batch   150  of  4,384.\n",
            "  Batch   200  of  4,384.\n",
            "  Batch   250  of  4,384.\n",
            "  Batch   300  of  4,384.\n",
            "  Batch   350  of  4,384.\n",
            "  Batch   400  of  4,384.\n",
            "  Batch   450  of  4,384.\n",
            "  Batch   500  of  4,384.\n",
            "  Batch   550  of  4,384.\n",
            "  Batch   600  of  4,384.\n",
            "  Batch   650  of  4,384.\n",
            "  Batch   700  of  4,384.\n",
            "  Batch   750  of  4,384.\n",
            "  Batch   800  of  4,384.\n",
            "  Batch   850  of  4,384.\n",
            "  Batch   900  of  4,384.\n",
            "  Batch   950  of  4,384.\n",
            "  Batch 1,000  of  4,384.\n",
            "  Batch 1,050  of  4,384.\n",
            "  Batch 1,100  of  4,384.\n",
            "  Batch 1,150  of  4,384.\n",
            "  Batch 1,200  of  4,384.\n",
            "  Batch 1,250  of  4,384.\n",
            "  Batch 1,300  of  4,384.\n",
            "  Batch 1,350  of  4,384.\n",
            "  Batch 1,400  of  4,384.\n",
            "  Batch 1,450  of  4,384.\n",
            "  Batch 1,500  of  4,384.\n",
            "  Batch 1,550  of  4,384.\n",
            "  Batch 1,600  of  4,384.\n",
            "  Batch 1,650  of  4,384.\n",
            "  Batch 1,700  of  4,384.\n",
            "  Batch 1,750  of  4,384.\n",
            "  Batch 1,800  of  4,384.\n",
            "  Batch 1,850  of  4,384.\n",
            "  Batch 1,900  of  4,384.\n",
            "  Batch 1,950  of  4,384.\n",
            "  Batch 2,000  of  4,384.\n",
            "  Batch 2,050  of  4,384.\n",
            "  Batch 2,100  of  4,384.\n",
            "  Batch 2,150  of  4,384.\n",
            "  Batch 2,200  of  4,384.\n",
            "  Batch 2,250  of  4,384.\n",
            "  Batch 2,300  of  4,384.\n",
            "  Batch 2,350  of  4,384.\n",
            "  Batch 2,400  of  4,384.\n",
            "  Batch 2,450  of  4,384.\n",
            "  Batch 2,500  of  4,384.\n",
            "  Batch 2,550  of  4,384.\n",
            "  Batch 2,600  of  4,384.\n",
            "  Batch 2,650  of  4,384.\n",
            "  Batch 2,700  of  4,384.\n",
            "  Batch 2,750  of  4,384.\n",
            "  Batch 2,800  of  4,384.\n",
            "  Batch 2,850  of  4,384.\n",
            "  Batch 2,900  of  4,384.\n",
            "  Batch 2,950  of  4,384.\n",
            "  Batch 3,000  of  4,384.\n",
            "  Batch 3,050  of  4,384.\n",
            "  Batch 3,100  of  4,384.\n",
            "  Batch 3,150  of  4,384.\n",
            "  Batch 3,200  of  4,384.\n",
            "  Batch 3,250  of  4,384.\n",
            "  Batch 3,300  of  4,384.\n",
            "  Batch 3,350  of  4,384.\n",
            "  Batch 3,400  of  4,384.\n",
            "  Batch 3,450  of  4,384.\n",
            "  Batch 3,500  of  4,384.\n",
            "  Batch 3,550  of  4,384.\n",
            "  Batch 3,600  of  4,384.\n",
            "  Batch 3,650  of  4,384.\n",
            "  Batch 3,700  of  4,384.\n",
            "  Batch 3,750  of  4,384.\n",
            "  Batch 3,800  of  4,384.\n",
            "  Batch 3,850  of  4,384.\n",
            "  Batch 3,900  of  4,384.\n",
            "  Batch 3,950  of  4,384.\n",
            "  Batch 4,000  of  4,384.\n",
            "  Batch 4,050  of  4,384.\n",
            "  Batch 4,100  of  4,384.\n",
            "  Batch 4,150  of  4,384.\n",
            "  Batch 4,200  of  4,384.\n",
            "  Batch 4,250  of  4,384.\n",
            "  Batch 4,300  of  4,384.\n",
            "  Batch 4,350  of  4,384.\n",
            "\n",
            "Training Loss: 0.459\n",
            "Validation Loss: 0.891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))\n",
        "model1 = BERT_Arch(bert)\n",
        "model1 = model1.to(device)\n",
        "model1.load_state_dict(torch.load(path))\n",
        "model2= BERT_Arch(bert)\n",
        "model2= model2.to(device)\n",
        "model2.load_state_dict(torch.load(path))\n",
        "model3= BERT_Arch(bert)\n",
        "model3= model3.to(device)\n",
        "model3.load_state_dict(torch.load(path))\n",
        "model4= BERT_Arch(bert)\n",
        "model4= model4.to(device)\n",
        "model4.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "6ixNVA5DVMrk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d3636ef-0a39-4275-dfe7-067995620bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning for 1 month"
      ],
      "metadata": {
        "id": "7sI4SdMOesLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,25):\n",
        "  train_text_cov=df1.loc[:int(i*0.6*100),'data'].values\n",
        "  train_labels_cov=df1.loc[:int(i*0.6*100),'label'].values\n",
        "  val_text_cov=df1.loc[int(i*0.6*100):int(i*0.8*100),'data'].values\n",
        "  val_labels_cov=df1.loc[int(i*0.6*100):int(i*0.8*100),\"label\"].values\n",
        "  test_text_cov=df1.loc[int(i*0.8*100):,'data'].values\n",
        "  test_labels_cov=df1.loc[int(i*0.8*100):,'label'].values\n",
        "  max_seq_len = 15\n",
        "  tokens_train = tokenizer.batch_encode_plus(\n",
        "      train_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text_cov.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "  tokens_test = tokenizer.batch_encode_plus(\n",
        "      test_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "  train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "  train_y = torch.tensor(train_labels_cov.tolist())\n",
        "  val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "  val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "  val_y = torch.tensor(val_labels_cov.tolist())\n",
        "  test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "  test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  batch_size = 2\n",
        "  train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "  best_valid_loss = float('inf')\n",
        "  train_losses=[]\n",
        "  valid_losses=[]\n",
        "  epochs=4\n",
        "  for epoch in range(epochs):\n",
        "      print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "      train_loss, _ = train()\n",
        "      valid_loss, _ = evaluate()\n",
        "      if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model1.state_dict(), 'saved_weights1.pt')\n",
        "      train_losses.append(train_loss)\n",
        "      valid_losses.append(valid_loss)\n",
        "      print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "      print(f'Validation Loss: {valid_loss:.3f}')\n",
        "  path = 'saved_weights1.pt'\n",
        "  model1.load_state_dict(torch.load(path))\n",
        "  with torch.no_grad():\n",
        "    preds = model1(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "  preds = np.argmax(preds, axis = 1)\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  print(classification_report(test_y, preds))\n"
      ],
      "metadata": {
        "id": "BzO22KIueoj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning for 2 months"
      ],
      "metadata": {
        "id": "2y0-aesweg9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2,26,2):\n",
        "  train_text_cov=df1.loc[:int(i*0.6*100),'data'].values\n",
        "  train_labels_cov=df1.loc[:int(i*0.6*100),'label'].values\n",
        "  val_text_cov=df1.loc[int(i*0.6*100):int(i*0.8*100),'data'].values\n",
        "  val_labels_cov=df1.loc[int(i*0.6*100):int(i*0.8*100),\"label\"].values\n",
        "  test_text_cov=df1.loc[int(i*0.8*100):,'data'].values\n",
        "  test_labels_cov=df1.loc[int(i*0.8*100):,'label'].values\n",
        "  max_seq_len = 15\n",
        "  tokens_train = tokenizer.batch_encode_plus(\n",
        "      train_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text_cov.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "  tokens_test = tokenizer.batch_encode_plus(\n",
        "      test_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "  train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "  train_y = torch.tensor(train_labels_cov.tolist())\n",
        "  val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "  val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "  val_y = torch.tensor(val_labels_cov.tolist())\n",
        "  test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "  test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  batch_size = 2\n",
        "  train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "  best_valid_loss = float('inf')\n",
        "  train_losses=[]\n",
        "  valid_losses=[]\n",
        "  epochs=4\n",
        "  for epoch in range(epochs):\n",
        "      print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "      train_loss, _ = train()\n",
        "      valid_loss, _ = evaluate()\n",
        "      if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model2.state_dict(), 'saved_weights2.pt')\n",
        "      train_losses.append(train_loss)\n",
        "      valid_losses.append(valid_loss)\n",
        "      print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "      print(f'Validation Loss: {valid_loss:.3f}')\n",
        "  path = 'saved_weights2.pt'\n",
        "  model2.load_state_dict(torch.load(path))\n",
        "  with torch.no_grad():\n",
        "    preds = model2(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "  preds = np.argmax(preds, axis = 1)\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  print(classification_report(test_y, preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vU4iA8Pl4PvO",
        "outputId": "3e1e196a-7b43-4ef6-f710-938809193872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of     61.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.573\n",
            "Validation Loss: 0.493\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of     61.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.538\n",
            "Validation Loss: 0.500\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of     61.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.562\n",
            "Validation Loss: 0.502\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of     61.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.508\n",
            "Validation Loss: 0.755\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-201-01be5757c81e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-186-e54da7ab978a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_id, mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;31m#pass the inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_hs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_hs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m         )\n\u001b[1;32m   1030\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m                 )\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         )\n\u001b[1;32m    538\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/activations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.98 GiB (GPU 0; 14.76 GiB total capacity; 7.64 GiB already allocated; 3.08 GiB free; 10.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning for 3 months"
      ],
      "metadata": {
        "id": "KY9OZcWSea4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3,27,3):\n",
        "  train_text_cov=df1.loc[:int(0.0416*i*0.6*df1.shape[0]),'data'].values\n",
        "  train_labels_cov=df1.loc[:int(0.0416*i*0.6*df1.shape[0]),'label'].values\n",
        "  val_text_cov=df1.loc[int(0.0416*i*0.6*df1.shape[0]):int(0.0416*i*0.8*df1.shape[0]),'data'].values\n",
        "  val_labels_cov=df1.loc[int(0.0416*i*0.6*df1.shape[0]):int(0.0416*i*0.8*df1.shape[0]),\"label\"].values\n",
        "  test_text_cov=df1.loc[int(0.0416*i*0.8*df1.shape[0]):,'data'].values\n",
        "  test_labels_cov=df1.loc[int(0.0416*i*0.8*df1.shape[0]):,'label'].values\n",
        "  max_seq_len = 15\n",
        "  tokens_train = tokenizer.batch_encode_plus(\n",
        "      train_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text_cov.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "  tokens_test = tokenizer.batch_encode_plus(\n",
        "      test_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "  train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "  train_y = torch.tensor(train_labels_cov.tolist())\n",
        "  val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "  val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "  val_y = torch.tensor(val_labels_cov.tolist())\n",
        "  test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "  test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  batch_size = 12\n",
        "  train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "  best_valid_loss = float('inf')\n",
        "  train_losses=[]\n",
        "  valid_losses=[]\n",
        "  epochs=4\n",
        "  for epoch in range(epochs):\n",
        "      print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "      train_loss, _ = train()\n",
        "      valid_loss, _ = evaluate()\n",
        "      if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model3.state_dict(), 'saved_weights3.pt')\n",
        "      train_losses.append(train_loss)\n",
        "      valid_losses.append(valid_loss)\n",
        "      print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "      print(f'Validation Loss: {valid_loss:.3f}')\n",
        "  path = 'saved_weights3.pt'\n",
        "  model3.load_state_dict(torch.load(path))\n",
        "  with torch.no_grad():\n",
        "    preds = model3(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "  preds = np.argmax(preds, axis = 1)\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwHfiaNoNNp8",
        "outputId": "682e4e9c-326d-4db6-b0d3-28e20ac01009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.662\n",
            "Validation Loss: 0.636\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.620\n",
            "Validation Loss: 0.589\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.591\n",
            "Validation Loss: 0.539\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.587\n",
            "Validation Loss: 0.538\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.69      0.74      7864\n",
            "           1       0.73      0.82      0.77      7921\n",
            "\n",
            "    accuracy                           0.76     15785\n",
            "   macro avg       0.76      0.76      0.76     15785\n",
            "weighted avg       0.76      0.76      0.76     15785\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 0.552\n",
            "Validation Loss: 0.485\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 0.516\n",
            "Validation Loss: 0.473\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 0.487\n",
            "Validation Loss: 0.450\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 0.472\n",
            "Validation Loss: 0.457\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.87      0.81      6996\n",
            "           1       0.85      0.74      0.79      7038\n",
            "\n",
            "    accuracy                           0.80     14034\n",
            "   macro avg       0.81      0.80      0.80     14034\n",
            "weighted avg       0.81      0.80      0.80     14034\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    329.\n",
            "  Batch   100  of    329.\n",
            "  Batch   150  of    329.\n",
            "  Batch   200  of    329.\n",
            "  Batch   250  of    329.\n",
            "  Batch   300  of    329.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Training Loss: 0.466\n",
            "Validation Loss: 0.415\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    329.\n",
            "  Batch   100  of    329.\n",
            "  Batch   150  of    329.\n",
            "  Batch   200  of    329.\n",
            "  Batch   250  of    329.\n",
            "  Batch   300  of    329.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Training Loss: 0.451\n",
            "Validation Loss: 0.397\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    329.\n",
            "  Batch   100  of    329.\n",
            "  Batch   150  of    329.\n",
            "  Batch   200  of    329.\n",
            "  Batch   250  of    329.\n",
            "  Batch   300  of    329.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Training Loss: 0.443\n",
            "Validation Loss: 0.376\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    329.\n",
            "  Batch   100  of    329.\n",
            "  Batch   150  of    329.\n",
            "  Batch   200  of    329.\n",
            "  Batch   250  of    329.\n",
            "  Batch   300  of    329.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Training Loss: 0.446\n",
            "Validation Loss: 0.370\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.84      6107\n",
            "           1       0.83      0.88      0.85      6176\n",
            "\n",
            "    accuracy                           0.85     12283\n",
            "   macro avg       0.85      0.85      0.85     12283\n",
            "weighted avg       0.85      0.85      0.85     12283\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.430\n",
            "Validation Loss: 0.358\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.420\n",
            "Validation Loss: 0.331\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.415\n",
            "Validation Loss: 0.324\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.411\n",
            "Validation Loss: 0.381\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.78      0.84      5252\n",
            "           1       0.81      0.92      0.86      5281\n",
            "\n",
            "    accuracy                           0.85     10533\n",
            "   macro avg       0.86      0.85      0.85     10533\n",
            "weighted avg       0.86      0.85      0.85     10533\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    548.\n",
            "  Batch   100  of    548.\n",
            "  Batch   150  of    548.\n",
            "  Batch   200  of    548.\n",
            "  Batch   250  of    548.\n",
            "  Batch   300  of    548.\n",
            "  Batch   350  of    548.\n",
            "  Batch   400  of    548.\n",
            "  Batch   450  of    548.\n",
            "  Batch   500  of    548.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    183.\n",
            "  Batch   100  of    183.\n",
            "  Batch   150  of    183.\n",
            "\n",
            "Training Loss: 0.410\n",
            "Validation Loss: 0.333\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    548.\n",
            "  Batch   100  of    548.\n",
            "  Batch   150  of    548.\n",
            "  Batch   200  of    548.\n",
            "  Batch   250  of    548.\n",
            "  Batch   300  of    548.\n",
            "  Batch   350  of    548.\n",
            "  Batch   400  of    548.\n",
            "  Batch   450  of    548.\n",
            "  Batch   500  of    548.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    183.\n",
            "  Batch   100  of    183.\n",
            "  Batch   150  of    183.\n",
            "\n",
            "Training Loss: 0.405\n",
            "Validation Loss: 0.343\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    548.\n",
            "  Batch   100  of    548.\n",
            "  Batch   150  of    548.\n",
            "  Batch   200  of    548.\n",
            "  Batch   250  of    548.\n",
            "  Batch   300  of    548.\n",
            "  Batch   350  of    548.\n",
            "  Batch   400  of    548.\n",
            "  Batch   450  of    548.\n",
            "  Batch   500  of    548.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    183.\n",
            "  Batch   100  of    183.\n",
            "  Batch   150  of    183.\n",
            "\n",
            "Training Loss: 0.398\n",
            "Validation Loss: 0.306\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    548.\n",
            "  Batch   100  of    548.\n",
            "  Batch   150  of    548.\n",
            "  Batch   200  of    548.\n",
            "  Batch   250  of    548.\n",
            "  Batch   300  of    548.\n",
            "  Batch   350  of    548.\n",
            "  Batch   400  of    548.\n",
            "  Batch   450  of    548.\n",
            "  Batch   500  of    548.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    183.\n",
            "  Batch   100  of    183.\n",
            "  Batch   150  of    183.\n",
            "\n",
            "Training Loss: 0.390\n",
            "Validation Loss: 0.308\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.85      0.86      4374\n",
            "           1       0.86      0.87      0.86      4408\n",
            "\n",
            "    accuracy                           0.86      8782\n",
            "   macro avg       0.86      0.86      0.86      8782\n",
            "weighted avg       0.86      0.86      0.86      8782\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    657.\n",
            "  Batch   100  of    657.\n",
            "  Batch   150  of    657.\n",
            "  Batch   200  of    657.\n",
            "  Batch   250  of    657.\n",
            "  Batch   300  of    657.\n",
            "  Batch   350  of    657.\n",
            "  Batch   400  of    657.\n",
            "  Batch   450  of    657.\n",
            "  Batch   500  of    657.\n",
            "  Batch   550  of    657.\n",
            "  Batch   600  of    657.\n",
            "  Batch   650  of    657.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Training Loss: 0.387\n",
            "Validation Loss: 0.343\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    657.\n",
            "  Batch   100  of    657.\n",
            "  Batch   150  of    657.\n",
            "  Batch   200  of    657.\n",
            "  Batch   250  of    657.\n",
            "  Batch   300  of    657.\n",
            "  Batch   350  of    657.\n",
            "  Batch   400  of    657.\n",
            "  Batch   450  of    657.\n",
            "  Batch   500  of    657.\n",
            "  Batch   550  of    657.\n",
            "  Batch   600  of    657.\n",
            "  Batch   650  of    657.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Training Loss: 0.385\n",
            "Validation Loss: 0.308\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    657.\n",
            "  Batch   100  of    657.\n",
            "  Batch   150  of    657.\n",
            "  Batch   200  of    657.\n",
            "  Batch   250  of    657.\n",
            "  Batch   300  of    657.\n",
            "  Batch   350  of    657.\n",
            "  Batch   400  of    657.\n",
            "  Batch   450  of    657.\n",
            "  Batch   500  of    657.\n",
            "  Batch   550  of    657.\n",
            "  Batch   600  of    657.\n",
            "  Batch   650  of    657.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Training Loss: 0.375\n",
            "Validation Loss: 0.412\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    657.\n",
            "  Batch   100  of    657.\n",
            "  Batch   150  of    657.\n",
            "  Batch   200  of    657.\n",
            "  Batch   250  of    657.\n",
            "  Batch   300  of    657.\n",
            "  Batch   350  of    657.\n",
            "  Batch   400  of    657.\n",
            "  Batch   450  of    657.\n",
            "  Batch   500  of    657.\n",
            "  Batch   550  of    657.\n",
            "  Batch   600  of    657.\n",
            "  Batch   650  of    657.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Training Loss: 0.381\n",
            "Validation Loss: 0.323\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.81      0.85      3509\n",
            "           1       0.83      0.91      0.87      3522\n",
            "\n",
            "    accuracy                           0.86      7031\n",
            "   macro avg       0.86      0.86      0.86      7031\n",
            "weighted avg       0.86      0.86      0.86      7031\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    766.\n",
            "  Batch   100  of    766.\n",
            "  Batch   150  of    766.\n",
            "  Batch   200  of    766.\n",
            "  Batch   250  of    766.\n",
            "  Batch   300  of    766.\n",
            "  Batch   350  of    766.\n",
            "  Batch   400  of    766.\n",
            "  Batch   450  of    766.\n",
            "  Batch   500  of    766.\n",
            "  Batch   550  of    766.\n",
            "  Batch   600  of    766.\n",
            "  Batch   650  of    766.\n",
            "  Batch   700  of    766.\n",
            "  Batch   750  of    766.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    256.\n",
            "  Batch   100  of    256.\n",
            "  Batch   150  of    256.\n",
            "  Batch   200  of    256.\n",
            "  Batch   250  of    256.\n",
            "\n",
            "Training Loss: 0.383\n",
            "Validation Loss: 0.319\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    766.\n",
            "  Batch   100  of    766.\n",
            "  Batch   150  of    766.\n",
            "  Batch   200  of    766.\n",
            "  Batch   250  of    766.\n",
            "  Batch   300  of    766.\n",
            "  Batch   350  of    766.\n",
            "  Batch   400  of    766.\n",
            "  Batch   450  of    766.\n",
            "  Batch   500  of    766.\n",
            "  Batch   550  of    766.\n",
            "  Batch   600  of    766.\n",
            "  Batch   650  of    766.\n",
            "  Batch   700  of    766.\n",
            "  Batch   750  of    766.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    256.\n",
            "  Batch   100  of    256.\n",
            "  Batch   150  of    256.\n",
            "  Batch   200  of    256.\n",
            "  Batch   250  of    256.\n",
            "\n",
            "Training Loss: 0.383\n",
            "Validation Loss: 0.310\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    766.\n",
            "  Batch   100  of    766.\n",
            "  Batch   150  of    766.\n",
            "  Batch   200  of    766.\n",
            "  Batch   250  of    766.\n",
            "  Batch   300  of    766.\n",
            "  Batch   350  of    766.\n",
            "  Batch   400  of    766.\n",
            "  Batch   450  of    766.\n",
            "  Batch   500  of    766.\n",
            "  Batch   550  of    766.\n",
            "  Batch   600  of    766.\n",
            "  Batch   650  of    766.\n",
            "  Batch   700  of    766.\n",
            "  Batch   750  of    766.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    256.\n",
            "  Batch   100  of    256.\n",
            "  Batch   150  of    256.\n",
            "  Batch   200  of    256.\n",
            "  Batch   250  of    256.\n",
            "\n",
            "Training Loss: 0.368\n",
            "Validation Loss: 0.314\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    766.\n",
            "  Batch   100  of    766.\n",
            "  Batch   150  of    766.\n",
            "  Batch   200  of    766.\n",
            "  Batch   250  of    766.\n",
            "  Batch   300  of    766.\n",
            "  Batch   350  of    766.\n",
            "  Batch   400  of    766.\n",
            "  Batch   450  of    766.\n",
            "  Batch   500  of    766.\n",
            "  Batch   550  of    766.\n",
            "  Batch   600  of    766.\n",
            "  Batch   650  of    766.\n",
            "  Batch   700  of    766.\n",
            "  Batch   750  of    766.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    256.\n",
            "  Batch   100  of    256.\n",
            "  Batch   150  of    256.\n",
            "  Batch   200  of    256.\n",
            "  Batch   250  of    256.\n",
            "\n",
            "Training Loss: 0.377\n",
            "Validation Loss: 0.304\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.86      0.87      2633\n",
            "           1       0.87      0.89      0.88      2648\n",
            "\n",
            "    accuracy                           0.88      5281\n",
            "   macro avg       0.88      0.88      0.88      5281\n",
            "weighted avg       0.88      0.88      0.88      5281\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.373\n",
            "Validation Loss: 0.313\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.373\n",
            "Validation Loss: 0.344\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.369\n",
            "Validation Loss: 0.300\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.356\n",
            "Validation Loss: 0.313\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.84      0.87      1764\n",
            "           1       0.85      0.91      0.88      1766\n",
            "\n",
            "    accuracy                           0.87      3530\n",
            "   macro avg       0.88      0.87      0.87      3530\n",
            "weighted avg       0.88      0.87      0.87      3530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning for 4 month quantum"
      ],
      "metadata": {
        "id": "l18CJaqveOMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4,28,4):\n",
        "  train_text_cov=df1.loc[:int(0.0416*i*0.6*df1.shape[0]),'data'].values\n",
        "  train_labels_cov=df1.loc[:int(0.0416*i*0.6*df1.shape[0]),'label'].values\n",
        "  val_text_cov=df1.loc[int(0.0416*i*0.6*df1.shape[0]):int(0.0416*i*0.8*df1.shape[0]),'data'].values\n",
        "  val_labels_cov=df1.loc[int(0.0416*i*0.6*df1.shape[0]):int(0.0416*i*0.8*df1.shape[0]),\"label\"].values\n",
        "  test_text_cov=df1.loc[int(0.0416*i*0.8*df1.shape[0]):,'data'].values\n",
        "  test_labels_cov=df1.loc[int(0.0416*i*0.8*df1.shape[0]):,'label'].values\n",
        "  max_seq_len = 15\n",
        "  tokens_train = tokenizer.batch_encode_plus(\n",
        "      train_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text_cov.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "  tokens_test = tokenizer.batch_encode_plus(\n",
        "      test_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "  train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "  train_y = torch.tensor(train_labels_cov.tolist())\n",
        "  val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "  val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "  val_y = torch.tensor(val_labels_cov.tolist())\n",
        "  test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "  test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  batch_size = 12\n",
        "  train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "  best_valid_loss = float('inf')\n",
        "  train_losses=[]\n",
        "  valid_losses=[]\n",
        "  epochs=4\n",
        "  for epoch in range(epochs):\n",
        "      print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "      train_loss, _ = train()\n",
        "      valid_loss, _ = evaluate()\n",
        "      if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model4.state_dict(), 'saved_weights4.pt')\n",
        "      train_losses.append(train_loss)\n",
        "      valid_losses.append(valid_loss)\n",
        "      print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "      print(f'Validation Loss: {valid_loss:.3f}')\n",
        "  path = 'saved_weights.pt'\n",
        "  model4.load_state_dict(torch.load(path))\n",
        "  with torch.no_grad():\n",
        "    preds = model4(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "  preds = np.argmax(preds, axis = 1)\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  print(classification_report(test_y, preds))):\n",
        "  "
      ],
      "metadata": {
        "id": "0BfdBDvS3N-N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}