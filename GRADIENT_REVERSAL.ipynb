{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRADIENT_REVERSAL",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "704ce632d4ea4f0b877e4143b4edad62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d57a7cf6bd34deab80446a128ba11f8",
              "IPY_MODEL_61ef9c8381df49d5afa26cbf900cbdcc",
              "IPY_MODEL_193af5bd2ebe40ecbe845f0ea60ab3ae"
            ],
            "layout": "IPY_MODEL_dc4fb5b01bc24ac4a94e03294d909cb5"
          }
        },
        "6d57a7cf6bd34deab80446a128ba11f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a4411cec3504023a6f8301523bb2cfb",
            "placeholder": "​",
            "style": "IPY_MODEL_a32e1d98bc5441aaa99da4282894267d",
            "value": "Downloading: 100%"
          }
        },
        "61ef9c8381df49d5afa26cbf900cbdcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e0a1fa0614340c3b9d3591374cb2930",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a75eda207982435e9cd60836b74f4c70",
            "value": 570
          }
        },
        "193af5bd2ebe40ecbe845f0ea60ab3ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04c30d8f31d742568a92586e078aa68c",
            "placeholder": "​",
            "style": "IPY_MODEL_ec66158ab674424e8dd0ee9ac25e20e9",
            "value": " 570/570 [00:00&lt;00:00, 15.4kB/s]"
          }
        },
        "dc4fb5b01bc24ac4a94e03294d909cb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a4411cec3504023a6f8301523bb2cfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a32e1d98bc5441aaa99da4282894267d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e0a1fa0614340c3b9d3591374cb2930": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a75eda207982435e9cd60836b74f4c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04c30d8f31d742568a92586e078aa68c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec66158ab674424e8dd0ee9ac25e20e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac63f898b7634a50befaf072076ebeb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f94adfea15b342b4b55a4fa4e16bf3f0",
              "IPY_MODEL_65bd939b1f0448b09a87fbe56c3d2e1b",
              "IPY_MODEL_b98086db831742d9b49fd2d897330e0b"
            ],
            "layout": "IPY_MODEL_9ee1bb903fd148eaafc58a9af87aaa94"
          }
        },
        "f94adfea15b342b4b55a4fa4e16bf3f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54914e2788044efab9db5146a8465cc5",
            "placeholder": "​",
            "style": "IPY_MODEL_1382d4df9e1d406dadfca31a171bd275",
            "value": "Downloading: 100%"
          }
        },
        "65bd939b1f0448b09a87fbe56c3d2e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_050b566392024c188d7510ff6ccc9cd8",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b1fe45cf2024e0d8c66e54e0e32c543",
            "value": 440473133
          }
        },
        "b98086db831742d9b49fd2d897330e0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00a892b5dba24125aefa9965eaf8bb64",
            "placeholder": "​",
            "style": "IPY_MODEL_b1137c67e294489dba4b8c1c5a33ee76",
            "value": " 420M/420M [00:14&lt;00:00, 18.3MB/s]"
          }
        },
        "9ee1bb903fd148eaafc58a9af87aaa94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54914e2788044efab9db5146a8465cc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1382d4df9e1d406dadfca31a171bd275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "050b566392024c188d7510ff6ccc9cd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b1fe45cf2024e0d8c66e54e0e32c543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00a892b5dba24125aefa9965eaf8bb64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1137c67e294489dba4b8c1c5a33ee76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee9431505ac548e0ac19248b75dcbfd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_839fd5f2514a471eb71d6ba0ad3d6dc4",
              "IPY_MODEL_9b518e0b4ca6477284c9e20ede37aa5f",
              "IPY_MODEL_f85cb4e53dcd4126bb6d67a23bc63aee"
            ],
            "layout": "IPY_MODEL_f6863ed425084c029eca95c59eb31bc5"
          }
        },
        "839fd5f2514a471eb71d6ba0ad3d6dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c2785f019004f0c9e25d34a75893359",
            "placeholder": "​",
            "style": "IPY_MODEL_55ac78e5abcc41c699a67ad431c7683b",
            "value": "Downloading: 100%"
          }
        },
        "9b518e0b4ca6477284c9e20ede37aa5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8ebe68251144eff8d27f9aee9d9e154",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4344ffca4a934858b516128f812ba696",
            "value": 28
          }
        },
        "f85cb4e53dcd4126bb6d67a23bc63aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95f98adf3dcd4e9ab8ebd7c331d12f7c",
            "placeholder": "​",
            "style": "IPY_MODEL_7d21e279154a40dda42a0e9592aa95e9",
            "value": " 28.0/28.0 [00:00&lt;00:00, 521B/s]"
          }
        },
        "f6863ed425084c029eca95c59eb31bc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c2785f019004f0c9e25d34a75893359": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55ac78e5abcc41c699a67ad431c7683b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8ebe68251144eff8d27f9aee9d9e154": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4344ffca4a934858b516128f812ba696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95f98adf3dcd4e9ab8ebd7c331d12f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d21e279154a40dda42a0e9592aa95e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40fe44a84d9c4c04afbc9af886f2a9ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f41db1ea9c34ce3bf2f86429b277853",
              "IPY_MODEL_1116049c47ee4d8c87980ffbb0921631",
              "IPY_MODEL_628d3d5a039845d49cb98ac2c2735615"
            ],
            "layout": "IPY_MODEL_75c9668082cb4cf9b7406317e840fb0b"
          }
        },
        "9f41db1ea9c34ce3bf2f86429b277853": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca7e073332334b1eaff206ba1770f3c4",
            "placeholder": "​",
            "style": "IPY_MODEL_60e83de3bbb94d8c8f805483f57b5502",
            "value": "Downloading: 100%"
          }
        },
        "1116049c47ee4d8c87980ffbb0921631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa5f1dec8c354e538f8bf394dfcdba56",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cdb12fe894d4d36a13cd3cac5a15b1b",
            "value": 231508
          }
        },
        "628d3d5a039845d49cb98ac2c2735615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86eabebfe4d8493a8811edb6647aa80b",
            "placeholder": "​",
            "style": "IPY_MODEL_bad892fd1abf4f8ca38327bb983450ad",
            "value": " 226k/226k [00:00&lt;00:00, 859kB/s]"
          }
        },
        "75c9668082cb4cf9b7406317e840fb0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca7e073332334b1eaff206ba1770f3c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60e83de3bbb94d8c8f805483f57b5502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa5f1dec8c354e538f8bf394dfcdba56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cdb12fe894d4d36a13cd3cac5a15b1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86eabebfe4d8493a8811edb6647aa80b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bad892fd1abf4f8ca38327bb983450ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c971676a81f4bf09265401c7165aa37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de2c60eb54324b578cfd12f4eb4129d3",
              "IPY_MODEL_fcfdda054c474fa6bff701da0b516426",
              "IPY_MODEL_8ac8ba92d0364332890d16aa9a430655"
            ],
            "layout": "IPY_MODEL_ae394e10473b462aa24b5b5125f0d1f3"
          }
        },
        "de2c60eb54324b578cfd12f4eb4129d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f004dbb404445a69959a1d69a03c352",
            "placeholder": "​",
            "style": "IPY_MODEL_37415e1588c2422fa369ccb65264724d",
            "value": "Downloading: 100%"
          }
        },
        "fcfdda054c474fa6bff701da0b516426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_581351370d364ee694f66affc3fcb668",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88b7623a2f6242d0bac782a6bbf40a6d",
            "value": 466062
          }
        },
        "8ac8ba92d0364332890d16aa9a430655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce0508b7330a4918915c0da4392746e8",
            "placeholder": "​",
            "style": "IPY_MODEL_d83f0c71c0df46dfa1e2672fa059b6e2",
            "value": " 455k/455k [00:00&lt;00:00, 874kB/s]"
          }
        },
        "ae394e10473b462aa24b5b5125f0d1f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f004dbb404445a69959a1d69a03c352": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37415e1588c2422fa369ccb65264724d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "581351370d364ee694f66affc3fcb668": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88b7623a2f6242d0bac782a6bbf40a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce0508b7330a4918915c0da4392746e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d83f0c71c0df46dfa1e2672fa059b6e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJuFCWD_q0ag",
        "outputId": "b4a6580b-85f5-473c-a8de-7a1b3e90ef88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 6.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 52.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "RZvj_NHcvPZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"mixed_domain_godamit.csv\",usecols=[\"data\",\"label\"])"
      ],
      "metadata": {
        "id": "ckb0u8tvvUAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=pd.read_csv(\"covid_kd_val.csv\",usecols=[\"data\",\"label\"])\n",
        "df2=pd.read_csv(\"covid_kd_test.csv\",usecols=[\"data\",\"label\"])"
      ],
      "metadata": {
        "id": "RLVl1XJ_pIDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_text_cov=df1.loc[:,\"data\"].values\n",
        "val_labels_cov=df1.loc[:,\"label\"].values\n",
        "test_text_cov=df2.loc[:,\"data\"].values\n",
        "test_labels_cov=df2.loc[:,\"label\"].values"
      ],
      "metadata": {
        "id": "_PyW08d5pLeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df.loc[:int(0.7*df.shape[0]),'data'],df.loc[:int(0.7*df.shape[0]):,'label'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df.loc[:0.7*int(df.shape[0]),'label'])"
      ],
      "metadata": {
        "id": "srhlauG6xTmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='bert-base-uncased'\n",
        "bert = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251,
          "referenced_widgets": [
            "704ce632d4ea4f0b877e4143b4edad62",
            "6d57a7cf6bd34deab80446a128ba11f8",
            "61ef9c8381df49d5afa26cbf900cbdcc",
            "193af5bd2ebe40ecbe845f0ea60ab3ae",
            "dc4fb5b01bc24ac4a94e03294d909cb5",
            "5a4411cec3504023a6f8301523bb2cfb",
            "a32e1d98bc5441aaa99da4282894267d",
            "9e0a1fa0614340c3b9d3591374cb2930",
            "a75eda207982435e9cd60836b74f4c70",
            "04c30d8f31d742568a92586e078aa68c",
            "ec66158ab674424e8dd0ee9ac25e20e9",
            "ac63f898b7634a50befaf072076ebeb8",
            "f94adfea15b342b4b55a4fa4e16bf3f0",
            "65bd939b1f0448b09a87fbe56c3d2e1b",
            "b98086db831742d9b49fd2d897330e0b",
            "9ee1bb903fd148eaafc58a9af87aaa94",
            "54914e2788044efab9db5146a8465cc5",
            "1382d4df9e1d406dadfca31a171bd275",
            "050b566392024c188d7510ff6ccc9cd8",
            "0b1fe45cf2024e0d8c66e54e0e32c543",
            "00a892b5dba24125aefa9965eaf8bb64",
            "b1137c67e294489dba4b8c1c5a33ee76",
            "ee9431505ac548e0ac19248b75dcbfd9",
            "839fd5f2514a471eb71d6ba0ad3d6dc4",
            "9b518e0b4ca6477284c9e20ede37aa5f",
            "f85cb4e53dcd4126bb6d67a23bc63aee",
            "f6863ed425084c029eca95c59eb31bc5",
            "2c2785f019004f0c9e25d34a75893359",
            "55ac78e5abcc41c699a67ad431c7683b",
            "b8ebe68251144eff8d27f9aee9d9e154",
            "4344ffca4a934858b516128f812ba696",
            "95f98adf3dcd4e9ab8ebd7c331d12f7c",
            "7d21e279154a40dda42a0e9592aa95e9",
            "40fe44a84d9c4c04afbc9af886f2a9ac",
            "9f41db1ea9c34ce3bf2f86429b277853",
            "1116049c47ee4d8c87980ffbb0921631",
            "628d3d5a039845d49cb98ac2c2735615",
            "75c9668082cb4cf9b7406317e840fb0b",
            "ca7e073332334b1eaff206ba1770f3c4",
            "60e83de3bbb94d8c8f805483f57b5502",
            "aa5f1dec8c354e538f8bf394dfcdba56",
            "3cdb12fe894d4d36a13cd3cac5a15b1b",
            "86eabebfe4d8493a8811edb6647aa80b",
            "bad892fd1abf4f8ca38327bb983450ad",
            "9c971676a81f4bf09265401c7165aa37",
            "de2c60eb54324b578cfd12f4eb4129d3",
            "fcfdda054c474fa6bff701da0b516426",
            "8ac8ba92d0364332890d16aa9a430655",
            "ae394e10473b462aa24b5b5125f0d1f3",
            "9f004dbb404445a69959a1d69a03c352",
            "37415e1588c2422fa369ccb65264724d",
            "581351370d364ee694f66affc3fcb668",
            "88b7623a2f6242d0bac782a6bbf40a6d",
            "ce0508b7330a4918915c0da4392746e8",
            "d83f0c71c0df46dfa1e2672fa059b6e2"
          ]
        },
        "id": "choFIcNCyja-",
        "outputId": "2a664862-99c9-4e58-95e0-45d9d849e35a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "704ce632d4ea4f0b877e4143b4edad62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac63f898b7634a50befaf072076ebeb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee9431505ac548e0ac19248b75dcbfd9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40fe44a84d9c4c04afbc9af886f2a9ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c971676a81f4bf09265401c7165aa37"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 15\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_vD14Q5y4sh",
        "outputId": "c6487062-a8ef-4a8a-ceac-56f7ba901fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text_cov.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYv43Tshy6nL",
        "outputId": "2e7685ce-3af8-4360-ef99-00a8d8ac319f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text_cov.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52SUMcpZy7OL",
        "outputId": "f67bb166-d4d0-478c-cd55-c3969a4d975c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())"
      ],
      "metadata": {
        "id": "ZdS6I9uRy-Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels_cov.tolist())"
      ],
      "metadata": {
        "id": "Cs_whLqyzB1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels_cov.tolist())"
      ],
      "metadata": {
        "id": "cDFK-RfezFhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 12\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "a41i-ZOKzHPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "qFA2bIIMzKDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Gradient Reversal"
      ],
      "metadata": {
        "id": "XXBSoFMVAkwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Function\n",
        "class GradientReversalFn(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "        return x.view_as(x)\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "        return output, None"
      ],
      "metadata": {
        "id": "ioFutbcZzUK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "      super(BERT_Arch, self).__init__()\n",
        "      self.bert = bert \n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.relu =  nn.ReLU()\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      self.label_layer=nn.Sequential(nn.Linear(512,2),nn.LogSoftmax(dim=1))\n",
        "      self.domain_layer=nn.Sequential(nn.Linear(512,2),nn.LogSoftmax(dim=1))\n",
        "    def forward(self, sent_id, mask,grl_lambda=1.0):\n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      x_reversed=GradientReversalFn.apply(x, grl_lambda)\n",
        "      x=self.label_layer(x)\n",
        "      x1=self.domain_layer(x_reversed)\n",
        "      return x,x1"
      ],
      "metadata": {
        "id": "_WczGJorzZSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERT_Arch(bert)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "hPUVwQhtzgoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePUorlSTzi58",
        "outputId": "65d5f894-28a6-41f6-b6a0-4fd4d2a25e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(train_labels),\n",
        "                                        y = train_labels                                                    \n",
        "                                    )\n",
        "class_weights = dict(zip(np.unique(train_labels), class_weights))\n"
      ],
      "metadata": {
        "id": "WW4rFsrfzk5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "li=[]\n",
        "li.append(class_weights[0])\n",
        "li.append(class_weights[1])"
      ],
      "metadata": {
        "id": "kzOoCfFkzuXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights= torch.tensor(li,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "V14MCgS2znML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy1  = nn.NLLLoss(weight=weights) \n",
        "cross_entropy2  = nn.NLLLoss(weight=weights) "
      ],
      "metadata": {
        "id": "LjX5Anb-9VKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  total_preds=[]\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    model.zero_grad()        \n",
        "    preds,preds1 = model(sent_id, mask)\n",
        "    loss = cross_entropy(preds, labels)\n",
        "    zeros_pred=torch.zeros(len(labels), dtype=torch.long).to(device)\n",
        "    ones_pred=torch.ones(len(labels), dtype=torch.long).to(device)\n",
        "    loss1 = cross_entropy1(preds1, zeros_pred)\n",
        "    loss2 = cross_entropy2(preds1, ones_pred)\n",
        "    total_loss =  loss+loss1+loss2\n",
        "    total_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "    total_preds.append(preds)\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "ehjYbC2F2Pue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  model.eval()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  total_preds = []\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "    batch = [t.to(device) for t in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    with torch.no_grad():\n",
        "      preds,_ = model(sent_id, mask)\n",
        "      loss = cross_entropy(preds,labels)\n",
        "      total_loss = total_loss + loss.item()\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "      total_preds.append(preds)\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "mBtcy8tS2d_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float('inf')\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "for epoch in range(epochs):\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    train_loss, _ = train()\n",
        "    valid_loss, _ = evaluate()\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVXdpORZ2xWb",
        "outputId": "1d0bd4ce-b0c5-46d1-d66e-5d060adcfa96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of  2,568.\n",
            "  Batch   100  of  2,568.\n",
            "  Batch   150  of  2,568.\n",
            "  Batch   200  of  2,568.\n",
            "  Batch   250  of  2,568.\n",
            "  Batch   300  of  2,568.\n",
            "  Batch   350  of  2,568.\n",
            "  Batch   400  of  2,568.\n",
            "  Batch   450  of  2,568.\n",
            "  Batch   500  of  2,568.\n",
            "  Batch   550  of  2,568.\n",
            "  Batch   600  of  2,568.\n",
            "  Batch   650  of  2,568.\n",
            "  Batch   700  of  2,568.\n",
            "  Batch   750  of  2,568.\n",
            "  Batch   800  of  2,568.\n",
            "  Batch   850  of  2,568.\n",
            "  Batch   900  of  2,568.\n",
            "  Batch   950  of  2,568.\n",
            "  Batch 1,000  of  2,568.\n",
            "  Batch 1,050  of  2,568.\n",
            "  Batch 1,100  of  2,568.\n",
            "  Batch 1,150  of  2,568.\n",
            "  Batch 1,200  of  2,568.\n",
            "  Batch 1,250  of  2,568.\n",
            "  Batch 1,300  of  2,568.\n",
            "  Batch 1,350  of  2,568.\n",
            "  Batch 1,400  of  2,568.\n",
            "  Batch 1,450  of  2,568.\n",
            "  Batch 1,500  of  2,568.\n",
            "  Batch 1,550  of  2,568.\n",
            "  Batch 1,600  of  2,568.\n",
            "  Batch 1,650  of  2,568.\n",
            "  Batch 1,700  of  2,568.\n",
            "  Batch 1,750  of  2,568.\n",
            "  Batch 1,800  of  2,568.\n",
            "  Batch 1,850  of  2,568.\n",
            "  Batch 1,900  of  2,568.\n",
            "  Batch 1,950  of  2,568.\n",
            "  Batch 2,000  of  2,568.\n",
            "  Batch 2,050  of  2,568.\n",
            "  Batch 2,100  of  2,568.\n",
            "  Batch 2,150  of  2,568.\n",
            "  Batch 2,200  of  2,568.\n",
            "  Batch 2,250  of  2,568.\n",
            "  Batch 2,300  of  2,568.\n",
            "  Batch 2,350  of  2,568.\n",
            "  Batch 2,400  of  2,568.\n",
            "  Batch 2,450  of  2,568.\n",
            "  Batch 2,500  of  2,568.\n",
            "  Batch 2,550  of  2,568.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  1,462.\n",
            "  Batch   100  of  1,462.\n",
            "  Batch   150  of  1,462.\n",
            "  Batch   200  of  1,462.\n",
            "  Batch   250  of  1,462.\n",
            "  Batch   300  of  1,462.\n",
            "  Batch   350  of  1,462.\n",
            "  Batch   400  of  1,462.\n",
            "  Batch   450  of  1,462.\n",
            "  Batch   500  of  1,462.\n",
            "  Batch   550  of  1,462.\n",
            "  Batch   600  of  1,462.\n",
            "  Batch   650  of  1,462.\n",
            "  Batch   700  of  1,462.\n",
            "  Batch   750  of  1,462.\n",
            "  Batch   800  of  1,462.\n",
            "  Batch   850  of  1,462.\n",
            "  Batch   900  of  1,462.\n",
            "  Batch   950  of  1,462.\n",
            "  Batch 1,000  of  1,462.\n",
            "  Batch 1,050  of  1,462.\n",
            "  Batch 1,100  of  1,462.\n",
            "  Batch 1,150  of  1,462.\n",
            "  Batch 1,200  of  1,462.\n",
            "  Batch 1,250  of  1,462.\n",
            "  Batch 1,300  of  1,462.\n",
            "  Batch 1,350  of  1,462.\n",
            "  Batch 1,400  of  1,462.\n",
            "  Batch 1,450  of  1,462.\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 1.615\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of  2,568.\n",
            "  Batch   100  of  2,568.\n",
            "  Batch   150  of  2,568.\n",
            "  Batch   200  of  2,568.\n",
            "  Batch   250  of  2,568.\n",
            "  Batch   300  of  2,568.\n",
            "  Batch   350  of  2,568.\n",
            "  Batch   400  of  2,568.\n",
            "  Batch   450  of  2,568.\n",
            "  Batch   500  of  2,568.\n",
            "  Batch   550  of  2,568.\n",
            "  Batch   600  of  2,568.\n",
            "  Batch   650  of  2,568.\n",
            "  Batch   700  of  2,568.\n",
            "  Batch   750  of  2,568.\n",
            "  Batch   800  of  2,568.\n",
            "  Batch   850  of  2,568.\n",
            "  Batch   900  of  2,568.\n",
            "  Batch   950  of  2,568.\n",
            "  Batch 1,000  of  2,568.\n",
            "  Batch 1,050  of  2,568.\n",
            "  Batch 1,100  of  2,568.\n",
            "  Batch 1,150  of  2,568.\n",
            "  Batch 1,200  of  2,568.\n",
            "  Batch 1,250  of  2,568.\n",
            "  Batch 1,300  of  2,568.\n",
            "  Batch 1,350  of  2,568.\n",
            "  Batch 1,400  of  2,568.\n",
            "  Batch 1,450  of  2,568.\n",
            "  Batch 1,500  of  2,568.\n",
            "  Batch 1,550  of  2,568.\n",
            "  Batch 1,600  of  2,568.\n",
            "  Batch 1,650  of  2,568.\n",
            "  Batch 1,700  of  2,568.\n",
            "  Batch 1,750  of  2,568.\n",
            "  Batch 1,800  of  2,568.\n",
            "  Batch 1,850  of  2,568.\n",
            "  Batch 1,900  of  2,568.\n",
            "  Batch 1,950  of  2,568.\n",
            "  Batch 2,000  of  2,568.\n",
            "  Batch 2,050  of  2,568.\n",
            "  Batch 2,100  of  2,568.\n",
            "  Batch 2,150  of  2,568.\n",
            "  Batch 2,200  of  2,568.\n",
            "  Batch 2,250  of  2,568.\n",
            "  Batch 2,300  of  2,568.\n",
            "  Batch 2,350  of  2,568.\n",
            "  Batch 2,400  of  2,568.\n",
            "  Batch 2,450  of  2,568.\n",
            "  Batch 2,500  of  2,568.\n",
            "  Batch 2,550  of  2,568.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  1,462.\n",
            "  Batch   100  of  1,462.\n",
            "  Batch   150  of  1,462.\n",
            "  Batch   200  of  1,462.\n",
            "  Batch   250  of  1,462.\n",
            "  Batch   300  of  1,462.\n",
            "  Batch   350  of  1,462.\n",
            "  Batch   400  of  1,462.\n",
            "  Batch   450  of  1,462.\n",
            "  Batch   500  of  1,462.\n",
            "  Batch   550  of  1,462.\n",
            "  Batch   600  of  1,462.\n",
            "  Batch   650  of  1,462.\n",
            "  Batch   700  of  1,462.\n",
            "  Batch   750  of  1,462.\n",
            "  Batch   800  of  1,462.\n",
            "  Batch   850  of  1,462.\n",
            "  Batch   900  of  1,462.\n",
            "  Batch   950  of  1,462.\n",
            "  Batch 1,000  of  1,462.\n",
            "  Batch 1,050  of  1,462.\n",
            "  Batch 1,100  of  1,462.\n",
            "  Batch 1,150  of  1,462.\n",
            "  Batch 1,200  of  1,462.\n",
            "  Batch 1,250  of  1,462.\n",
            "  Batch 1,300  of  1,462.\n",
            "  Batch 1,350  of  1,462.\n",
            "  Batch 1,400  of  1,462.\n",
            "  Batch 1,450  of  1,462.\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 1.158\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of  2,568.\n",
            "  Batch   100  of  2,568.\n",
            "  Batch   150  of  2,568.\n",
            "  Batch   200  of  2,568.\n",
            "  Batch   250  of  2,568.\n",
            "  Batch   300  of  2,568.\n",
            "  Batch   350  of  2,568.\n",
            "  Batch   400  of  2,568.\n",
            "  Batch   450  of  2,568.\n",
            "  Batch   500  of  2,568.\n",
            "  Batch   550  of  2,568.\n",
            "  Batch   600  of  2,568.\n",
            "  Batch   650  of  2,568.\n",
            "  Batch   700  of  2,568.\n",
            "  Batch   750  of  2,568.\n",
            "  Batch   800  of  2,568.\n",
            "  Batch   850  of  2,568.\n",
            "  Batch   900  of  2,568.\n",
            "  Batch   950  of  2,568.\n",
            "  Batch 1,000  of  2,568.\n",
            "  Batch 1,050  of  2,568.\n",
            "  Batch 1,100  of  2,568.\n",
            "  Batch 1,150  of  2,568.\n",
            "  Batch 1,200  of  2,568.\n",
            "  Batch 1,250  of  2,568.\n",
            "  Batch 1,300  of  2,568.\n",
            "  Batch 1,350  of  2,568.\n",
            "  Batch 1,400  of  2,568.\n",
            "  Batch 1,450  of  2,568.\n",
            "  Batch 1,500  of  2,568.\n",
            "  Batch 1,550  of  2,568.\n",
            "  Batch 1,600  of  2,568.\n",
            "  Batch 1,650  of  2,568.\n",
            "  Batch 1,700  of  2,568.\n",
            "  Batch 1,750  of  2,568.\n",
            "  Batch 1,800  of  2,568.\n",
            "  Batch 1,850  of  2,568.\n",
            "  Batch 1,900  of  2,568.\n",
            "  Batch 1,950  of  2,568.\n",
            "  Batch 2,000  of  2,568.\n",
            "  Batch 2,050  of  2,568.\n",
            "  Batch 2,100  of  2,568.\n",
            "  Batch 2,150  of  2,568.\n",
            "  Batch 2,200  of  2,568.\n",
            "  Batch 2,250  of  2,568.\n",
            "  Batch 2,300  of  2,568.\n",
            "  Batch 2,350  of  2,568.\n",
            "  Batch 2,400  of  2,568.\n",
            "  Batch 2,450  of  2,568.\n",
            "  Batch 2,500  of  2,568.\n",
            "  Batch 2,550  of  2,568.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  1,462.\n",
            "  Batch   100  of  1,462.\n",
            "  Batch   150  of  1,462.\n",
            "  Batch   200  of  1,462.\n",
            "  Batch   250  of  1,462.\n",
            "  Batch   300  of  1,462.\n",
            "  Batch   350  of  1,462.\n",
            "  Batch   400  of  1,462.\n",
            "  Batch   450  of  1,462.\n",
            "  Batch   500  of  1,462.\n",
            "  Batch   550  of  1,462.\n",
            "  Batch   600  of  1,462.\n",
            "  Batch   650  of  1,462.\n",
            "  Batch   700  of  1,462.\n",
            "  Batch   750  of  1,462.\n",
            "  Batch   800  of  1,462.\n",
            "  Batch   850  of  1,462.\n",
            "  Batch   900  of  1,462.\n",
            "  Batch   950  of  1,462.\n",
            "  Batch 1,000  of  1,462.\n",
            "  Batch 1,050  of  1,462.\n",
            "  Batch 1,100  of  1,462.\n",
            "  Batch 1,150  of  1,462.\n",
            "  Batch 1,200  of  1,462.\n",
            "  Batch 1,250  of  1,462.\n",
            "  Batch 1,300  of  1,462.\n",
            "  Batch 1,350  of  1,462.\n",
            "  Batch 1,400  of  1,462.\n",
            "  Batch 1,450  of  1,462.\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 1.245\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of  2,568.\n",
            "  Batch   100  of  2,568.\n",
            "  Batch   150  of  2,568.\n",
            "  Batch   200  of  2,568.\n",
            "  Batch   250  of  2,568.\n",
            "  Batch   300  of  2,568.\n",
            "  Batch   350  of  2,568.\n",
            "  Batch   400  of  2,568.\n",
            "  Batch   450  of  2,568.\n",
            "  Batch   500  of  2,568.\n",
            "  Batch   550  of  2,568.\n",
            "  Batch   600  of  2,568.\n",
            "  Batch   650  of  2,568.\n",
            "  Batch   700  of  2,568.\n",
            "  Batch   750  of  2,568.\n",
            "  Batch   800  of  2,568.\n",
            "  Batch   850  of  2,568.\n",
            "  Batch   900  of  2,568.\n",
            "  Batch   950  of  2,568.\n",
            "  Batch 1,000  of  2,568.\n",
            "  Batch 1,050  of  2,568.\n",
            "  Batch 1,100  of  2,568.\n",
            "  Batch 1,150  of  2,568.\n",
            "  Batch 1,200  of  2,568.\n",
            "  Batch 1,250  of  2,568.\n",
            "  Batch 1,300  of  2,568.\n",
            "  Batch 1,350  of  2,568.\n",
            "  Batch 1,400  of  2,568.\n",
            "  Batch 1,450  of  2,568.\n",
            "  Batch 1,500  of  2,568.\n",
            "  Batch 1,550  of  2,568.\n",
            "  Batch 1,600  of  2,568.\n",
            "  Batch 1,650  of  2,568.\n",
            "  Batch 1,700  of  2,568.\n",
            "  Batch 1,750  of  2,568.\n",
            "  Batch 1,800  of  2,568.\n",
            "  Batch 1,850  of  2,568.\n",
            "  Batch 1,900  of  2,568.\n",
            "  Batch 1,950  of  2,568.\n",
            "  Batch 2,000  of  2,568.\n",
            "  Batch 2,050  of  2,568.\n",
            "  Batch 2,100  of  2,568.\n",
            "  Batch 2,150  of  2,568.\n",
            "  Batch 2,200  of  2,568.\n",
            "  Batch 2,250  of  2,568.\n",
            "  Batch 2,300  of  2,568.\n",
            "  Batch 2,350  of  2,568.\n",
            "  Batch 2,400  of  2,568.\n",
            "  Batch 2,450  of  2,568.\n",
            "  Batch 2,500  of  2,568.\n",
            "  Batch 2,550  of  2,568.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  1,462.\n",
            "  Batch   100  of  1,462.\n",
            "  Batch   150  of  1,462.\n",
            "  Batch   200  of  1,462.\n",
            "  Batch   250  of  1,462.\n",
            "  Batch   300  of  1,462.\n",
            "  Batch   350  of  1,462.\n",
            "  Batch   400  of  1,462.\n",
            "  Batch   450  of  1,462.\n",
            "  Batch   500  of  1,462.\n",
            "  Batch   550  of  1,462.\n",
            "  Batch   600  of  1,462.\n",
            "  Batch   650  of  1,462.\n",
            "  Batch   700  of  1,462.\n",
            "  Batch   750  of  1,462.\n",
            "  Batch   800  of  1,462.\n",
            "  Batch   850  of  1,462.\n",
            "  Batch   900  of  1,462.\n",
            "  Batch   950  of  1,462.\n",
            "  Batch 1,000  of  1,462.\n",
            "  Batch 1,050  of  1,462.\n",
            "  Batch 1,100  of  1,462.\n",
            "  Batch 1,150  of  1,462.\n",
            "  Batch 1,200  of  1,462.\n",
            "  Batch 1,250  of  1,462.\n",
            "  Batch 1,300  of  1,462.\n",
            "  Batch 1,350  of  1,462.\n",
            "  Batch 1,400  of  1,462.\n",
            "  Batch 1,450  of  1,462.\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 1.147\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of  2,568.\n",
            "  Batch   100  of  2,568.\n",
            "  Batch   150  of  2,568.\n",
            "  Batch   200  of  2,568.\n",
            "  Batch   250  of  2,568.\n",
            "  Batch   300  of  2,568.\n",
            "  Batch   350  of  2,568.\n",
            "  Batch   400  of  2,568.\n",
            "  Batch   450  of  2,568.\n",
            "  Batch   500  of  2,568.\n",
            "  Batch   550  of  2,568.\n",
            "  Batch   600  of  2,568.\n",
            "  Batch   650  of  2,568.\n",
            "  Batch   700  of  2,568.\n",
            "  Batch   750  of  2,568.\n",
            "  Batch   800  of  2,568.\n",
            "  Batch   850  of  2,568.\n",
            "  Batch   900  of  2,568.\n",
            "  Batch   950  of  2,568.\n",
            "  Batch 1,000  of  2,568.\n",
            "  Batch 1,050  of  2,568.\n",
            "  Batch 1,100  of  2,568.\n",
            "  Batch 1,150  of  2,568.\n",
            "  Batch 1,200  of  2,568.\n",
            "  Batch 1,250  of  2,568.\n",
            "  Batch 1,300  of  2,568.\n",
            "  Batch 1,350  of  2,568.\n",
            "  Batch 1,400  of  2,568.\n",
            "  Batch 1,450  of  2,568.\n",
            "  Batch 1,500  of  2,568.\n",
            "  Batch 1,550  of  2,568.\n",
            "  Batch 1,600  of  2,568.\n",
            "  Batch 1,650  of  2,568.\n",
            "  Batch 1,700  of  2,568.\n",
            "  Batch 1,750  of  2,568.\n",
            "  Batch 1,800  of  2,568.\n",
            "  Batch 1,850  of  2,568.\n",
            "  Batch 1,900  of  2,568.\n",
            "  Batch 1,950  of  2,568.\n",
            "  Batch 2,000  of  2,568.\n",
            "  Batch 2,050  of  2,568.\n",
            "  Batch 2,100  of  2,568.\n",
            "  Batch 2,150  of  2,568.\n",
            "  Batch 2,200  of  2,568.\n",
            "  Batch 2,250  of  2,568.\n",
            "  Batch 2,300  of  2,568.\n",
            "  Batch 2,350  of  2,568.\n",
            "  Batch 2,400  of  2,568.\n",
            "  Batch 2,450  of  2,568.\n",
            "  Batch 2,500  of  2,568.\n",
            "  Batch 2,550  of  2,568.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  1,462.\n",
            "  Batch   100  of  1,462.\n",
            "  Batch   150  of  1,462.\n",
            "  Batch   200  of  1,462.\n",
            "  Batch   250  of  1,462.\n",
            "  Batch   300  of  1,462.\n",
            "  Batch   350  of  1,462.\n",
            "  Batch   400  of  1,462.\n",
            "  Batch   450  of  1,462.\n",
            "  Batch   500  of  1,462.\n",
            "  Batch   550  of  1,462.\n",
            "  Batch   600  of  1,462.\n",
            "  Batch   650  of  1,462.\n",
            "  Batch   700  of  1,462.\n",
            "  Batch   750  of  1,462.\n",
            "  Batch   800  of  1,462.\n",
            "  Batch   850  of  1,462.\n",
            "  Batch   900  of  1,462.\n",
            "  Batch   950  of  1,462.\n",
            "  Batch 1,000  of  1,462.\n",
            "  Batch 1,050  of  1,462.\n",
            "  Batch 1,100  of  1,462.\n",
            "  Batch 1,150  of  1,462.\n",
            "  Batch 1,200  of  1,462.\n",
            "  Batch 1,250  of  1,462.\n",
            "  Batch 1,300  of  1,462.\n",
            "  Batch 1,350  of  1,462.\n",
            "  Batch 1,400  of  1,462.\n",
            "  Batch 1,450  of  1,462.\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 1.089\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of  2,568.\n",
            "  Batch   100  of  2,568.\n",
            "  Batch   150  of  2,568.\n",
            "  Batch   200  of  2,568.\n",
            "  Batch   250  of  2,568.\n",
            "  Batch   300  of  2,568.\n",
            "  Batch   350  of  2,568.\n",
            "  Batch   400  of  2,568.\n",
            "  Batch   450  of  2,568.\n",
            "  Batch   500  of  2,568.\n",
            "  Batch   550  of  2,568.\n",
            "  Batch   600  of  2,568.\n",
            "  Batch   650  of  2,568.\n",
            "  Batch   700  of  2,568.\n",
            "  Batch   750  of  2,568.\n",
            "  Batch   800  of  2,568.\n",
            "  Batch   850  of  2,568.\n",
            "  Batch   900  of  2,568.\n",
            "  Batch   950  of  2,568.\n",
            "  Batch 1,000  of  2,568.\n",
            "  Batch 1,050  of  2,568.\n",
            "  Batch 1,100  of  2,568.\n",
            "  Batch 1,150  of  2,568.\n",
            "  Batch 1,200  of  2,568.\n",
            "  Batch 1,250  of  2,568.\n",
            "  Batch 1,300  of  2,568.\n",
            "  Batch 1,350  of  2,568.\n",
            "  Batch 1,400  of  2,568.\n",
            "  Batch 1,450  of  2,568.\n",
            "  Batch 1,500  of  2,568.\n",
            "  Batch 1,550  of  2,568.\n",
            "  Batch 1,600  of  2,568.\n",
            "  Batch 1,650  of  2,568.\n",
            "  Batch 1,700  of  2,568.\n",
            "  Batch 1,750  of  2,568.\n",
            "  Batch 1,800  of  2,568.\n",
            "  Batch 1,850  of  2,568.\n",
            "  Batch 1,900  of  2,568.\n",
            "  Batch 1,950  of  2,568.\n",
            "  Batch 2,000  of  2,568.\n",
            "  Batch 2,050  of  2,568.\n",
            "  Batch 2,100  of  2,568.\n",
            "  Batch 2,150  of  2,568.\n",
            "  Batch 2,200  of  2,568.\n",
            "  Batch 2,250  of  2,568.\n",
            "  Batch 2,300  of  2,568.\n",
            "  Batch 2,350  of  2,568.\n",
            "  Batch 2,400  of  2,568.\n",
            "  Batch 2,450  of  2,568.\n",
            "  Batch 2,500  of  2,568.\n",
            "  Batch 2,550  of  2,568.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  1,462.\n",
            "  Batch   100  of  1,462.\n",
            "  Batch   150  of  1,462.\n",
            "  Batch   200  of  1,462.\n",
            "  Batch   250  of  1,462.\n",
            "  Batch   300  of  1,462.\n",
            "  Batch   350  of  1,462.\n",
            "  Batch   400  of  1,462.\n",
            "  Batch   450  of  1,462.\n",
            "  Batch   500  of  1,462.\n",
            "  Batch   550  of  1,462.\n",
            "  Batch   600  of  1,462.\n",
            "  Batch   650  of  1,462.\n",
            "  Batch   700  of  1,462.\n",
            "  Batch   750  of  1,462.\n",
            "  Batch   800  of  1,462.\n",
            "  Batch   850  of  1,462.\n",
            "  Batch   900  of  1,462.\n",
            "  Batch   950  of  1,462.\n",
            "  Batch 1,000  of  1,462.\n",
            "  Batch 1,050  of  1,462.\n",
            "  Batch 1,100  of  1,462.\n",
            "  Batch 1,150  of  1,462.\n",
            "  Batch 1,200  of  1,462.\n",
            "  Batch 1,250  of  1,462.\n",
            "  Batch 1,300  of  1,462.\n",
            "  Batch 1,350  of  1,462.\n",
            "  Batch 1,400  of  1,462.\n",
            "  Batch 1,450  of  1,462.\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 1.391\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of  2,568.\n",
            "  Batch   100  of  2,568.\n",
            "  Batch   150  of  2,568.\n",
            "  Batch   200  of  2,568.\n",
            "  Batch   250  of  2,568.\n",
            "  Batch   300  of  2,568.\n",
            "  Batch   350  of  2,568.\n",
            "  Batch   400  of  2,568.\n",
            "  Batch   450  of  2,568.\n",
            "  Batch   500  of  2,568.\n",
            "  Batch   550  of  2,568.\n",
            "  Batch   600  of  2,568.\n",
            "  Batch   650  of  2,568.\n",
            "  Batch   700  of  2,568.\n",
            "  Batch   750  of  2,568.\n",
            "  Batch   800  of  2,568.\n",
            "  Batch   850  of  2,568.\n",
            "  Batch   900  of  2,568.\n",
            "  Batch   950  of  2,568.\n",
            "  Batch 1,000  of  2,568.\n",
            "  Batch 1,050  of  2,568.\n",
            "  Batch 1,100  of  2,568.\n",
            "  Batch 1,150  of  2,568.\n",
            "  Batch 1,200  of  2,568.\n",
            "  Batch 1,250  of  2,568.\n",
            "  Batch 1,300  of  2,568.\n",
            "  Batch 1,350  of  2,568.\n",
            "  Batch 1,400  of  2,568.\n",
            "  Batch 1,450  of  2,568.\n",
            "  Batch 1,500  of  2,568.\n",
            "  Batch 1,550  of  2,568.\n",
            "  Batch 1,600  of  2,568.\n",
            "  Batch 1,650  of  2,568.\n",
            "  Batch 1,700  of  2,568.\n",
            "  Batch 1,750  of  2,568.\n",
            "  Batch 1,800  of  2,568.\n",
            "  Batch 1,850  of  2,568.\n",
            "  Batch 1,900  of  2,568.\n",
            "  Batch 1,950  of  2,568.\n",
            "  Batch 2,000  of  2,568.\n",
            "  Batch 2,050  of  2,568.\n",
            "  Batch 2,100  of  2,568.\n",
            "  Batch 2,150  of  2,568.\n",
            "  Batch 2,200  of  2,568.\n",
            "  Batch 2,250  of  2,568.\n",
            "  Batch 2,300  of  2,568.\n",
            "  Batch 2,350  of  2,568.\n",
            "  Batch 2,400  of  2,568.\n",
            "  Batch 2,450  of  2,568.\n",
            "  Batch 2,500  of  2,568.\n",
            "  Batch 2,550  of  2,568.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  1,462.\n",
            "  Batch   100  of  1,462.\n",
            "  Batch   150  of  1,462.\n",
            "  Batch   200  of  1,462.\n",
            "  Batch   250  of  1,462.\n",
            "  Batch   300  of  1,462.\n",
            "  Batch   350  of  1,462.\n",
            "  Batch   400  of  1,462.\n",
            "  Batch   450  of  1,462.\n",
            "  Batch   500  of  1,462.\n",
            "  Batch   550  of  1,462.\n",
            "  Batch   600  of  1,462.\n",
            "  Batch   650  of  1,462.\n",
            "  Batch   700  of  1,462.\n",
            "  Batch   750  of  1,462.\n",
            "  Batch   800  of  1,462.\n",
            "  Batch   850  of  1,462.\n",
            "  Batch   900  of  1,462.\n",
            "  Batch   950  of  1,462.\n",
            "  Batch 1,000  of  1,462.\n",
            "  Batch 1,050  of  1,462.\n",
            "  Batch 1,100  of  1,462.\n",
            "  Batch 1,150  of  1,462.\n",
            "  Batch 1,200  of  1,462.\n",
            "  Batch 1,250  of  1,462.\n",
            "  Batch 1,300  of  1,462.\n",
            "  Batch 1,350  of  1,462.\n",
            "  Batch 1,400  of  1,462.\n",
            "  Batch 1,450  of  1,462.\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 1.441\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of  2,568.\n",
            "  Batch   100  of  2,568.\n",
            "  Batch   150  of  2,568.\n",
            "  Batch   200  of  2,568.\n",
            "  Batch   250  of  2,568.\n",
            "  Batch   300  of  2,568.\n",
            "  Batch   350  of  2,568.\n",
            "  Batch   400  of  2,568.\n",
            "  Batch   450  of  2,568.\n",
            "  Batch   500  of  2,568.\n",
            "  Batch   550  of  2,568.\n",
            "  Batch   600  of  2,568.\n",
            "  Batch   650  of  2,568.\n",
            "  Batch   700  of  2,568.\n",
            "  Batch   750  of  2,568.\n",
            "  Batch   800  of  2,568.\n",
            "  Batch   850  of  2,568.\n",
            "  Batch   900  of  2,568.\n",
            "  Batch   950  of  2,568.\n",
            "  Batch 1,000  of  2,568.\n",
            "  Batch 1,050  of  2,568.\n",
            "  Batch 1,100  of  2,568.\n",
            "  Batch 1,150  of  2,568.\n",
            "  Batch 1,200  of  2,568.\n",
            "  Batch 1,250  of  2,568.\n",
            "  Batch 1,300  of  2,568.\n",
            "  Batch 1,350  of  2,568.\n",
            "  Batch 1,400  of  2,568.\n",
            "  Batch 1,450  of  2,568.\n",
            "  Batch 1,500  of  2,568.\n",
            "  Batch 1,550  of  2,568.\n",
            "  Batch 1,600  of  2,568.\n",
            "  Batch 1,650  of  2,568.\n",
            "  Batch 1,700  of  2,568.\n",
            "  Batch 1,750  of  2,568.\n",
            "  Batch 1,800  of  2,568.\n",
            "  Batch 1,850  of  2,568.\n",
            "  Batch 1,900  of  2,568.\n",
            "  Batch 1,950  of  2,568.\n",
            "  Batch 2,000  of  2,568.\n",
            "  Batch 2,050  of  2,568.\n",
            "  Batch 2,100  of  2,568.\n",
            "  Batch 2,150  of  2,568.\n",
            "  Batch 2,200  of  2,568.\n",
            "  Batch 2,250  of  2,568.\n",
            "  Batch 2,300  of  2,568.\n",
            "  Batch 2,350  of  2,568.\n",
            "  Batch 2,400  of  2,568.\n",
            "  Batch 2,450  of  2,568.\n",
            "  Batch 2,500  of  2,568.\n",
            "  Batch 2,550  of  2,568.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  1,462.\n",
            "  Batch   100  of  1,462.\n",
            "  Batch   150  of  1,462.\n",
            "  Batch   200  of  1,462.\n",
            "  Batch   250  of  1,462.\n",
            "  Batch   300  of  1,462.\n",
            "  Batch   350  of  1,462.\n",
            "  Batch   400  of  1,462.\n",
            "  Batch   450  of  1,462.\n",
            "  Batch   500  of  1,462.\n",
            "  Batch   550  of  1,462.\n",
            "  Batch   600  of  1,462.\n",
            "  Batch   650  of  1,462.\n",
            "  Batch   700  of  1,462.\n",
            "  Batch   750  of  1,462.\n",
            "  Batch   800  of  1,462.\n",
            "  Batch   850  of  1,462.\n",
            "  Batch   900  of  1,462.\n",
            "  Batch   950  of  1,462.\n",
            "  Batch 1,000  of  1,462.\n",
            "  Batch 1,050  of  1,462.\n",
            "  Batch 1,100  of  1,462.\n",
            "  Batch 1,150  of  1,462.\n",
            "  Batch 1,200  of  1,462.\n",
            "  Batch 1,250  of  1,462.\n",
            "  Batch 1,300  of  1,462.\n",
            "  Batch 1,350  of  1,462.\n",
            "  Batch 1,400  of  1,462.\n",
            "  Batch 1,450  of  1,462.\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.909\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of  2,568.\n",
            "  Batch   100  of  2,568.\n",
            "  Batch   150  of  2,568.\n",
            "  Batch   200  of  2,568.\n",
            "  Batch   250  of  2,568.\n",
            "  Batch   300  of  2,568.\n",
            "  Batch   350  of  2,568.\n",
            "  Batch   400  of  2,568.\n",
            "  Batch   450  of  2,568.\n",
            "  Batch   500  of  2,568.\n",
            "  Batch   550  of  2,568.\n",
            "  Batch   600  of  2,568.\n",
            "  Batch   650  of  2,568.\n",
            "  Batch   700  of  2,568.\n",
            "  Batch   750  of  2,568.\n",
            "  Batch   800  of  2,568.\n",
            "  Batch   850  of  2,568.\n",
            "  Batch   900  of  2,568.\n",
            "  Batch   950  of  2,568.\n",
            "  Batch 1,000  of  2,568.\n",
            "  Batch 1,050  of  2,568.\n",
            "  Batch 1,100  of  2,568.\n",
            "  Batch 1,150  of  2,568.\n",
            "  Batch 1,200  of  2,568.\n",
            "  Batch 1,250  of  2,568.\n",
            "  Batch 1,300  of  2,568.\n",
            "  Batch 1,350  of  2,568.\n",
            "  Batch 1,400  of  2,568.\n",
            "  Batch 1,450  of  2,568.\n",
            "  Batch 1,500  of  2,568.\n",
            "  Batch 1,550  of  2,568.\n",
            "  Batch 1,600  of  2,568.\n",
            "  Batch 1,650  of  2,568.\n",
            "  Batch 1,700  of  2,568.\n",
            "  Batch 1,750  of  2,568.\n",
            "  Batch 1,800  of  2,568.\n",
            "  Batch 1,850  of  2,568.\n",
            "  Batch 1,900  of  2,568.\n",
            "  Batch 1,950  of  2,568.\n",
            "  Batch 2,000  of  2,568.\n",
            "  Batch 2,050  of  2,568.\n",
            "  Batch 2,100  of  2,568.\n",
            "  Batch 2,150  of  2,568.\n",
            "  Batch 2,200  of  2,568.\n",
            "  Batch 2,250  of  2,568.\n",
            "  Batch 2,300  of  2,568.\n",
            "  Batch 2,350  of  2,568.\n",
            "  Batch 2,400  of  2,568.\n",
            "  Batch 2,450  of  2,568.\n",
            "  Batch 2,500  of  2,568.\n",
            "  Batch 2,550  of  2,568.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  1,462.\n",
            "  Batch   100  of  1,462.\n",
            "  Batch   150  of  1,462.\n",
            "  Batch   200  of  1,462.\n",
            "  Batch   250  of  1,462.\n",
            "  Batch   300  of  1,462.\n",
            "  Batch   350  of  1,462.\n",
            "  Batch   400  of  1,462.\n",
            "  Batch   450  of  1,462.\n",
            "  Batch   500  of  1,462.\n",
            "  Batch   550  of  1,462.\n",
            "  Batch   600  of  1,462.\n",
            "  Batch   650  of  1,462.\n",
            "  Batch   700  of  1,462.\n",
            "  Batch   750  of  1,462.\n",
            "  Batch   800  of  1,462.\n",
            "  Batch   850  of  1,462.\n",
            "  Batch   900  of  1,462.\n",
            "  Batch   950  of  1,462.\n",
            "  Batch 1,000  of  1,462.\n",
            "  Batch 1,050  of  1,462.\n",
            "  Batch 1,100  of  1,462.\n",
            "  Batch 1,150  of  1,462.\n",
            "  Batch 1,200  of  1,462.\n",
            "  Batch 1,250  of  1,462.\n",
            "  Batch 1,300  of  1,462.\n",
            "  Batch 1,350  of  1,462.\n",
            "  Batch 1,400  of  1,462.\n",
            "  Batch 1,450  of  1,462.\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 1.156\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of  2,568.\n",
            "  Batch   100  of  2,568.\n",
            "  Batch   150  of  2,568.\n",
            "  Batch   200  of  2,568.\n",
            "  Batch   250  of  2,568.\n",
            "  Batch   300  of  2,568.\n",
            "  Batch   350  of  2,568.\n",
            "  Batch   400  of  2,568.\n",
            "  Batch   450  of  2,568.\n",
            "  Batch   500  of  2,568.\n",
            "  Batch   550  of  2,568.\n",
            "  Batch   600  of  2,568.\n",
            "  Batch   650  of  2,568.\n",
            "  Batch   700  of  2,568.\n",
            "  Batch   750  of  2,568.\n",
            "  Batch   800  of  2,568.\n",
            "  Batch   850  of  2,568.\n",
            "  Batch   900  of  2,568.\n",
            "  Batch   950  of  2,568.\n",
            "  Batch 1,000  of  2,568.\n",
            "  Batch 1,050  of  2,568.\n",
            "  Batch 1,100  of  2,568.\n",
            "  Batch 1,150  of  2,568.\n",
            "  Batch 1,200  of  2,568.\n",
            "  Batch 1,250  of  2,568.\n",
            "  Batch 1,300  of  2,568.\n",
            "  Batch 1,350  of  2,568.\n",
            "  Batch 1,400  of  2,568.\n",
            "  Batch 1,450  of  2,568.\n",
            "  Batch 1,500  of  2,568.\n",
            "  Batch 1,550  of  2,568.\n",
            "  Batch 1,600  of  2,568.\n",
            "  Batch 1,650  of  2,568.\n",
            "  Batch 1,700  of  2,568.\n",
            "  Batch 1,750  of  2,568.\n",
            "  Batch 1,800  of  2,568.\n",
            "  Batch 1,850  of  2,568.\n",
            "  Batch 1,900  of  2,568.\n",
            "  Batch 1,950  of  2,568.\n",
            "  Batch 2,000  of  2,568.\n",
            "  Batch 2,050  of  2,568.\n",
            "  Batch 2,100  of  2,568.\n",
            "  Batch 2,150  of  2,568.\n",
            "  Batch 2,200  of  2,568.\n",
            "  Batch 2,250  of  2,568.\n",
            "  Batch 2,300  of  2,568.\n",
            "  Batch 2,350  of  2,568.\n",
            "  Batch 2,400  of  2,568.\n",
            "  Batch 2,450  of  2,568.\n",
            "  Batch 2,500  of  2,568.\n",
            "  Batch 2,550  of  2,568.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  1,462.\n",
            "  Batch   100  of  1,462.\n",
            "  Batch   150  of  1,462.\n",
            "  Batch   200  of  1,462.\n",
            "  Batch   250  of  1,462.\n",
            "  Batch   300  of  1,462.\n",
            "  Batch   350  of  1,462.\n",
            "  Batch   400  of  1,462.\n",
            "  Batch   450  of  1,462.\n",
            "  Batch   500  of  1,462.\n",
            "  Batch   550  of  1,462.\n",
            "  Batch   600  of  1,462.\n",
            "  Batch   650  of  1,462.\n",
            "  Batch   700  of  1,462.\n",
            "  Batch   750  of  1,462.\n",
            "  Batch   800  of  1,462.\n",
            "  Batch   850  of  1,462.\n",
            "  Batch   900  of  1,462.\n",
            "  Batch   950  of  1,462.\n",
            "  Batch 1,000  of  1,462.\n",
            "  Batch 1,050  of  1,462.\n",
            "  Batch 1,100  of  1,462.\n",
            "  Batch 1,150  of  1,462.\n",
            "  Batch 1,200  of  1,462.\n",
            "  Batch 1,250  of  1,462.\n",
            "  Batch 1,300  of  1,462.\n",
            "  Batch 1,350  of  1,462.\n",
            "  Batch 1,400  of  1,462.\n",
            "  Batch 1,450  of  1,462.\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 1.048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))\n",
        "model1 = BERT_Arch(bert)\n",
        "model1 = model1.to(device)\n",
        "model1.load_state_dict(torch.load(path))\n",
        "model2= BERT_Arch(bert)\n",
        "model2= model2.to(device)\n",
        "model2.load_state_dict(torch.load(path))\n",
        "model3= BERT_Arch(bert)\n",
        "model3= model3.to(device)\n",
        "model3.load_state_dict(torch.load(path))\n",
        "model4= BERT_Arch(bert)\n",
        "model4= model4.to(device)\n",
        "model4.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "6hMtb8Yp-fcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning for time quantum of 1 month"
      ],
      "metadata": {
        "id": "1GKZJnU2681y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,25):\n",
        "  train_text_cov=df1.loc[:int(0.0416*i*0.6*df1.shape[0]),'data'].values\n",
        "  train_labels_cov=df1.loc[:int(0.0416*i*0.6*df1.shape[0]),'label'].values\n",
        "  val_text_cov=df1.loc[int(0.0416*i*0.6*df1.shape[0]):int(0.0416*i*0.8*df1.shape[0]),'data'].values\n",
        "  val_labels_cov=df1.loc[int(0.0416*i*0.6*df1.shape[0]):int(0.0416*i*0.8*df1.shape[0]),\"label\"].values\n",
        "  test_text_cov=df1.loc[int(0.0416*i*0.8*df1.shape[0]):,'data'].values\n",
        "  test_labels_cov=df1.loc[int(0.0416*i*0.8*df1.shape[0]):,'label'].values\n",
        "  max_seq_len = 15\n",
        "  tokens_train = tokenizer.batch_encode_plus(\n",
        "      train_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text_cov.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "  tokens_test = tokenizer.batch_encode_plus(\n",
        "      test_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "  train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "  train_y = torch.tensor(train_labels_cov.tolist())\n",
        "  val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "  val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "  val_y = torch.tensor(val_labels_cov.tolist())\n",
        "  test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "  test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  batch_size = 12\n",
        "  train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "  best_valid_loss = float('inf')\n",
        "  train_losses=[]\n",
        "  valid_losses=[]\n",
        "  epochs=4\n",
        "  for epoch in range(epochs):\n",
        "      print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "      train_loss, _ = train()\n",
        "      valid_loss, _ = evaluate()\n",
        "      if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "      train_losses.append(train_loss)\n",
        "      valid_losses.append(valid_loss)\n",
        "      print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "      print(f'Validation Loss: {valid_loss:.3f}')\n",
        "  path = 'saved_weights1.pt'\n",
        "  model1.load_state_dict(torch.load(path))\n",
        "  with torch.no_grad():\n",
        "    preds,_ = model1(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "  preds = np.argmax(preds, axis = 1)\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KLCqIhE4f3J",
        "outputId": "5aeef9d4-22b1-40f9-9ce5-df2ebcb2ef55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.015\n",
            "Validation Loss: 0.688\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.014\n",
            "Validation Loss: 0.572\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.014\n",
            "Validation Loss: 0.536\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.013\n",
            "Validation Loss: 0.471\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.78      0.79      7567\n",
            "           1       0.79      0.79      0.79      7634\n",
            "\n",
            "    accuracy                           0.79     15201\n",
            "   macro avg       0.79      0.79      0.79     15201\n",
            "weighted avg       0.79      0.79      0.79     15201\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.008\n",
            "Validation Loss: 0.494\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.006\n",
            "Validation Loss: 0.408\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.007\n",
            "Validation Loss: 0.376\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.006\n",
            "Validation Loss: 0.382\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84      6348\n",
            "           1       0.86      0.81      0.83      6519\n",
            "\n",
            "    accuracy                           0.84     12867\n",
            "   macro avg       0.84      0.84      0.84     12867\n",
            "weighted avg       0.84      0.84      0.84     12867\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.361\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.394\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.368\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.319\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.83      0.85      5181\n",
            "           1       0.84      0.87      0.86      5352\n",
            "\n",
            "    accuracy                           0.85     10533\n",
            "   macro avg       0.85      0.85      0.85     10533\n",
            "weighted avg       0.85      0.85      0.85     10533\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    584.\n",
            "  Batch   100  of    584.\n",
            "  Batch   150  of    584.\n",
            "  Batch   200  of    584.\n",
            "  Batch   250  of    584.\n",
            "  Batch   300  of    584.\n",
            "  Batch   350  of    584.\n",
            "  Batch   400  of    584.\n",
            "  Batch   450  of    584.\n",
            "  Batch   500  of    584.\n",
            "  Batch   550  of    584.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    195.\n",
            "  Batch   100  of    195.\n",
            "  Batch   150  of    195.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.357\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    584.\n",
            "  Batch   100  of    584.\n",
            "  Batch   150  of    584.\n",
            "  Batch   200  of    584.\n",
            "  Batch   250  of    584.\n",
            "  Batch   300  of    584.\n",
            "  Batch   350  of    584.\n",
            "  Batch   400  of    584.\n",
            "  Batch   450  of    584.\n",
            "  Batch   500  of    584.\n",
            "  Batch   550  of    584.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    195.\n",
            "  Batch   100  of    195.\n",
            "  Batch   150  of    195.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.341\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    584.\n",
            "  Batch   100  of    584.\n",
            "  Batch   150  of    584.\n",
            "  Batch   200  of    584.\n",
            "  Batch   250  of    584.\n",
            "  Batch   300  of    584.\n",
            "  Batch   350  of    584.\n",
            "  Batch   400  of    584.\n",
            "  Batch   450  of    584.\n",
            "  Batch   500  of    584.\n",
            "  Batch   550  of    584.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    195.\n",
            "  Batch   100  of    195.\n",
            "  Batch   150  of    195.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.358\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    584.\n",
            "  Batch   100  of    584.\n",
            "  Batch   150  of    584.\n",
            "  Batch   200  of    584.\n",
            "  Batch   250  of    584.\n",
            "  Batch   300  of    584.\n",
            "  Batch   350  of    584.\n",
            "  Batch   400  of    584.\n",
            "  Batch   450  of    584.\n",
            "  Batch   500  of    584.\n",
            "  Batch   550  of    584.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    195.\n",
            "  Batch   100  of    195.\n",
            "  Batch   150  of    195.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.337\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.82      0.86      4016\n",
            "           1       0.84      0.92      0.88      4182\n",
            "\n",
            "    accuracy                           0.87      8198\n",
            "   macro avg       0.87      0.87      0.87      8198\n",
            "weighted avg       0.87      0.87      0.87      8198\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    730.\n",
            "  Batch   100  of    730.\n",
            "  Batch   150  of    730.\n",
            "  Batch   200  of    730.\n",
            "  Batch   250  of    730.\n",
            "  Batch   300  of    730.\n",
            "  Batch   350  of    730.\n",
            "  Batch   400  of    730.\n",
            "  Batch   450  of    730.\n",
            "  Batch   500  of    730.\n",
            "  Batch   550  of    730.\n",
            "  Batch   600  of    730.\n",
            "  Batch   650  of    730.\n",
            "  Batch   700  of    730.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    244.\n",
            "  Batch   100  of    244.\n",
            "  Batch   150  of    244.\n",
            "  Batch   200  of    244.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.343\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    730.\n",
            "  Batch   100  of    730.\n",
            "  Batch   150  of    730.\n",
            "  Batch   200  of    730.\n",
            "  Batch   250  of    730.\n",
            "  Batch   300  of    730.\n",
            "  Batch   350  of    730.\n",
            "  Batch   400  of    730.\n",
            "  Batch   450  of    730.\n",
            "  Batch   500  of    730.\n",
            "  Batch   550  of    730.\n",
            "  Batch   600  of    730.\n",
            "  Batch   650  of    730.\n",
            "  Batch   700  of    730.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    244.\n",
            "  Batch   100  of    244.\n",
            "  Batch   150  of    244.\n",
            "  Batch   200  of    244.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.352\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    730.\n",
            "  Batch   100  of    730.\n",
            "  Batch   150  of    730.\n",
            "  Batch   200  of    730.\n",
            "  Batch   250  of    730.\n",
            "  Batch   300  of    730.\n",
            "  Batch   350  of    730.\n",
            "  Batch   400  of    730.\n",
            "  Batch   450  of    730.\n",
            "  Batch   500  of    730.\n",
            "  Batch   550  of    730.\n",
            "  Batch   600  of    730.\n",
            "  Batch   650  of    730.\n",
            "  Batch   700  of    730.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    244.\n",
            "  Batch   100  of    244.\n",
            "  Batch   150  of    244.\n",
            "  Batch   200  of    244.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.326\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    730.\n",
            "  Batch   100  of    730.\n",
            "  Batch   150  of    730.\n",
            "  Batch   200  of    730.\n",
            "  Batch   250  of    730.\n",
            "  Batch   300  of    730.\n",
            "  Batch   350  of    730.\n",
            "  Batch   400  of    730.\n",
            "  Batch   450  of    730.\n",
            "  Batch   500  of    730.\n",
            "  Batch   550  of    730.\n",
            "  Batch   600  of    730.\n",
            "  Batch   650  of    730.\n",
            "  Batch   700  of    730.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    244.\n",
            "  Batch   100  of    244.\n",
            "  Batch   150  of    244.\n",
            "  Batch   200  of    244.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.335\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87      2894\n",
            "           1       0.89      0.85      0.87      2970\n",
            "\n",
            "    accuracy                           0.87      5864\n",
            "   macro avg       0.87      0.87      0.87      5864\n",
            "weighted avg       0.87      0.87      0.87      5864\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.297\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.301\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.289\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.286\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.84      0.87      1740\n",
            "           1       0.85      0.92      0.88      1790\n",
            "\n",
            "    accuracy                           0.88      3530\n",
            "   macro avg       0.88      0.88      0.88      3530\n",
            "weighted avg       0.88      0.88      0.88      3530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_8jeaBp56480"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning for time quantum of 2 months"
      ],
      "metadata": {
        "id": "keHarURB7GAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2,26,2):\n",
        "  train_text_cov=df1.loc[:int(0.0416*i*0.6*df1.shape[0]),'data'].values\n",
        "  train_labels_cov=df1.loc[:int(0.0416*i*0.6*df1.shape[0]),'label'].values\n",
        "  val_text_cov=df1.loc[int(0.0416*i*0.6*df1.shape[0]):int(0.0416*i*0.8*df1.shape[0]),'data'].values\n",
        "  val_labels_cov=df1.loc[int(0.0416*i*0.6*df1.shape[0]):int(0.0416*i*0.8*df1.shape[0]),\"label\"].values\n",
        "  test_text_cov=df1.loc[int(0.0416*i*0.8*df1.shape[0]):,'data'].values\n",
        "  test_labels_cov=df1.loc[int(0.0416*i*0.8*df1.shape[0]):,'label'].values\n",
        "  max_seq_len = 15\n",
        "  tokens_train = tokenizer.batch_encode_plus(\n",
        "      train_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text_cov.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "  tokens_test = tokenizer.batch_encode_plus(\n",
        "      test_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "  train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "  train_y = torch.tensor(train_labels_cov.tolist())\n",
        "  val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "  val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "  val_y = torch.tensor(val_labels_cov.tolist())\n",
        "  test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "  test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  batch_size = 12\n",
        "  train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "  best_valid_loss = float('inf')\n",
        "  train_losses=[]\n",
        "  valid_losses=[]\n",
        "  epochs=4\n",
        "  for epoch in range(epochs):\n",
        "      print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "      train_loss, _ = train()\n",
        "      valid_loss, _ = evaluate()\n",
        "      if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model2.state_dict(), 'saved_weights2.pt')\n",
        "      train_losses.append(train_loss)\n",
        "      valid_losses.append(valid_loss)\n",
        "      print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "      print(f'Validation Loss: {valid_loss:.3f}')\n",
        "  path = 'saved_weights2.pt'\n",
        "  model2.load_state_dict(torch.load(path))\n",
        "  with torch.no_grad():\n",
        "    preds,_ = model2(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "  preds = np.argmax(preds, axis = 1)\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djE4omDF7lpu",
        "outputId": "9f594079-db53-4c9f-9b73-d56f9b4c25da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.030\n",
            "Validation Loss: 0.654\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.031\n",
            "Validation Loss: 0.637\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.027\n",
            "Validation Loss: 0.607\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.028\n",
            "Validation Loss: 0.609\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.69      0.68      8128\n",
            "           1       0.68      0.66      0.67      8240\n",
            "\n",
            "    accuracy                           0.67     16368\n",
            "   macro avg       0.67      0.67      0.67     16368\n",
            "weighted avg       0.67      0.67      0.67     16368\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.013\n",
            "Validation Loss: 0.593\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.014\n",
            "Validation Loss: 0.547\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.015\n",
            "Validation Loss: 0.524\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.012\n",
            "Validation Loss: 0.510\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.74      0.75      7567\n",
            "           1       0.75      0.77      0.76      7634\n",
            "\n",
            "    accuracy                           0.75     15201\n",
            "   macro avg       0.75      0.75      0.75     15201\n",
            "weighted avg       0.75      0.75      0.75     15201\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 0.008\n",
            "Validation Loss: 0.469\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 0.008\n",
            "Validation Loss: 0.446\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 0.008\n",
            "Validation Loss: 0.406\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 0.008\n",
            "Validation Loss: 0.398\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83      6969\n",
            "           1       0.85      0.80      0.83      7065\n",
            "\n",
            "    accuracy                           0.83     14034\n",
            "   macro avg       0.83      0.83      0.83     14034\n",
            "weighted avg       0.83      0.83      0.83     14034\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.007\n",
            "Validation Loss: 0.381\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.006\n",
            "Validation Loss: 0.386\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.006\n",
            "Validation Loss: 0.415\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.006\n",
            "Validation Loss: 0.365\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.85      6348\n",
            "           1       0.89      0.80      0.84      6519\n",
            "\n",
            "    accuracy                           0.85     12867\n",
            "   macro avg       0.85      0.85      0.85     12867\n",
            "weighted avg       0.85      0.85      0.85     12867\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    365.\n",
            "  Batch   100  of    365.\n",
            "  Batch   150  of    365.\n",
            "  Batch   200  of    365.\n",
            "  Batch   250  of    365.\n",
            "  Batch   300  of    365.\n",
            "  Batch   350  of    365.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    122.\n",
            "  Batch   100  of    122.\n",
            "\n",
            "Training Loss: 0.005\n",
            "Validation Loss: 0.321\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    365.\n",
            "  Batch   100  of    365.\n",
            "  Batch   150  of    365.\n",
            "  Batch   200  of    365.\n",
            "  Batch   250  of    365.\n",
            "  Batch   300  of    365.\n",
            "  Batch   350  of    365.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    122.\n",
            "  Batch   100  of    122.\n",
            "\n",
            "Training Loss: 0.006\n",
            "Validation Loss: 0.314\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    365.\n",
            "  Batch   100  of    365.\n",
            "  Batch   150  of    365.\n",
            "  Batch   200  of    365.\n",
            "  Batch   250  of    365.\n",
            "  Batch   300  of    365.\n",
            "  Batch   350  of    365.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    122.\n",
            "  Batch   100  of    122.\n",
            "\n",
            "Training Loss: 0.006\n",
            "Validation Loss: 0.324\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    365.\n",
            "  Batch   100  of    365.\n",
            "  Batch   150  of    365.\n",
            "  Batch   200  of    365.\n",
            "  Batch   250  of    365.\n",
            "  Batch   300  of    365.\n",
            "  Batch   350  of    365.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    122.\n",
            "  Batch   100  of    122.\n",
            "\n",
            "Training Loss: 0.005\n",
            "Validation Loss: 0.312\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86      5763\n",
            "           1       0.87      0.85      0.86      5937\n",
            "\n",
            "    accuracy                           0.86     11700\n",
            "   macro avg       0.86      0.86      0.86     11700\n",
            "weighted avg       0.86      0.86      0.86     11700\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.358\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.318\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.323\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.314\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.81      0.85      5181\n",
            "           1       0.83      0.91      0.87      5352\n",
            "\n",
            "    accuracy                           0.86     10533\n",
            "   macro avg       0.87      0.86      0.86     10533\n",
            "weighted avg       0.87      0.86      0.86     10533\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    511.\n",
            "  Batch   100  of    511.\n",
            "  Batch   150  of    511.\n",
            "  Batch   200  of    511.\n",
            "  Batch   250  of    511.\n",
            "  Batch   300  of    511.\n",
            "  Batch   350  of    511.\n",
            "  Batch   400  of    511.\n",
            "  Batch   450  of    511.\n",
            "  Batch   500  of    511.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    171.\n",
            "  Batch   100  of    171.\n",
            "  Batch   150  of    171.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.323\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    511.\n",
            "  Batch   100  of    511.\n",
            "  Batch   150  of    511.\n",
            "  Batch   200  of    511.\n",
            "  Batch   250  of    511.\n",
            "  Batch   300  of    511.\n",
            "  Batch   350  of    511.\n",
            "  Batch   400  of    511.\n",
            "  Batch   450  of    511.\n",
            "  Batch   500  of    511.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    171.\n",
            "  Batch   100  of    171.\n",
            "  Batch   150  of    171.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.333\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    511.\n",
            "  Batch   100  of    511.\n",
            "  Batch   150  of    511.\n",
            "  Batch   200  of    511.\n",
            "  Batch   250  of    511.\n",
            "  Batch   300  of    511.\n",
            "  Batch   350  of    511.\n",
            "  Batch   400  of    511.\n",
            "  Batch   450  of    511.\n",
            "  Batch   500  of    511.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    171.\n",
            "  Batch   100  of    171.\n",
            "  Batch   150  of    171.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.336\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    511.\n",
            "  Batch   100  of    511.\n",
            "  Batch   150  of    511.\n",
            "  Batch   200  of    511.\n",
            "  Batch   250  of    511.\n",
            "  Batch   300  of    511.\n",
            "  Batch   350  of    511.\n",
            "  Batch   400  of    511.\n",
            "  Batch   450  of    511.\n",
            "  Batch   500  of    511.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    171.\n",
            "  Batch   100  of    171.\n",
            "  Batch   150  of    171.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.320\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.82      0.86      4590\n",
            "           1       0.84      0.92      0.88      4776\n",
            "\n",
            "    accuracy                           0.87      9366\n",
            "   macro avg       0.87      0.87      0.87      9366\n",
            "weighted avg       0.87      0.87      0.87      9366\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    584.\n",
            "  Batch   100  of    584.\n",
            "  Batch   150  of    584.\n",
            "  Batch   200  of    584.\n",
            "  Batch   250  of    584.\n",
            "  Batch   300  of    584.\n",
            "  Batch   350  of    584.\n",
            "  Batch   400  of    584.\n",
            "  Batch   450  of    584.\n",
            "  Batch   500  of    584.\n",
            "  Batch   550  of    584.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    195.\n",
            "  Batch   100  of    195.\n",
            "  Batch   150  of    195.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.357\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    584.\n",
            "  Batch   100  of    584.\n",
            "  Batch   150  of    584.\n",
            "  Batch   200  of    584.\n",
            "  Batch   250  of    584.\n",
            "  Batch   300  of    584.\n",
            "  Batch   350  of    584.\n",
            "  Batch   400  of    584.\n",
            "  Batch   450  of    584.\n",
            "  Batch   500  of    584.\n",
            "  Batch   550  of    584.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    195.\n",
            "  Batch   100  of    195.\n",
            "  Batch   150  of    195.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.349\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    584.\n",
            "  Batch   100  of    584.\n",
            "  Batch   150  of    584.\n",
            "  Batch   200  of    584.\n",
            "  Batch   250  of    584.\n",
            "  Batch   300  of    584.\n",
            "  Batch   350  of    584.\n",
            "  Batch   400  of    584.\n",
            "  Batch   450  of    584.\n",
            "  Batch   500  of    584.\n",
            "  Batch   550  of    584.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    195.\n",
            "  Batch   100  of    195.\n",
            "  Batch   150  of    195.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.311\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    584.\n",
            "  Batch   100  of    584.\n",
            "  Batch   150  of    584.\n",
            "  Batch   200  of    584.\n",
            "  Batch   250  of    584.\n",
            "  Batch   300  of    584.\n",
            "  Batch   350  of    584.\n",
            "  Batch   400  of    584.\n",
            "  Batch   450  of    584.\n",
            "  Batch   500  of    584.\n",
            "  Batch   550  of    584.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    195.\n",
            "  Batch   100  of    195.\n",
            "  Batch   150  of    195.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.319\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.85      0.87      4016\n",
            "           1       0.87      0.90      0.88      4182\n",
            "\n",
            "    accuracy                           0.88      8198\n",
            "   macro avg       0.88      0.88      0.88      8198\n",
            "weighted avg       0.88      0.88      0.88      8198\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    657.\n",
            "  Batch   100  of    657.\n",
            "  Batch   150  of    657.\n",
            "  Batch   200  of    657.\n",
            "  Batch   250  of    657.\n",
            "  Batch   300  of    657.\n",
            "  Batch   350  of    657.\n",
            "  Batch   400  of    657.\n",
            "  Batch   450  of    657.\n",
            "  Batch   500  of    657.\n",
            "  Batch   550  of    657.\n",
            "  Batch   600  of    657.\n",
            "  Batch   650  of    657.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.358\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    657.\n",
            "  Batch   100  of    657.\n",
            "  Batch   150  of    657.\n",
            "  Batch   200  of    657.\n",
            "  Batch   250  of    657.\n",
            "  Batch   300  of    657.\n",
            "  Batch   350  of    657.\n",
            "  Batch   400  of    657.\n",
            "  Batch   450  of    657.\n",
            "  Batch   500  of    657.\n",
            "  Batch   550  of    657.\n",
            "  Batch   600  of    657.\n",
            "  Batch   650  of    657.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.328\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    657.\n",
            "  Batch   100  of    657.\n",
            "  Batch   150  of    657.\n",
            "  Batch   200  of    657.\n",
            "  Batch   250  of    657.\n",
            "  Batch   300  of    657.\n",
            "  Batch   350  of    657.\n",
            "  Batch   400  of    657.\n",
            "  Batch   450  of    657.\n",
            "  Batch   500  of    657.\n",
            "  Batch   550  of    657.\n",
            "  Batch   600  of    657.\n",
            "  Batch   650  of    657.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.333\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    657.\n",
            "  Batch   100  of    657.\n",
            "  Batch   150  of    657.\n",
            "  Batch   200  of    657.\n",
            "  Batch   250  of    657.\n",
            "  Batch   300  of    657.\n",
            "  Batch   350  of    657.\n",
            "  Batch   400  of    657.\n",
            "  Batch   450  of    657.\n",
            "  Batch   500  of    657.\n",
            "  Batch   550  of    657.\n",
            "  Batch   600  of    657.\n",
            "  Batch   650  of    657.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.323\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.83      0.87      3453\n",
            "           1       0.85      0.92      0.88      3578\n",
            "\n",
            "    accuracy                           0.87      7031\n",
            "   macro avg       0.88      0.87      0.87      7031\n",
            "weighted avg       0.88      0.87      0.87      7031\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    730.\n",
            "  Batch   100  of    730.\n",
            "  Batch   150  of    730.\n",
            "  Batch   200  of    730.\n",
            "  Batch   250  of    730.\n",
            "  Batch   300  of    730.\n",
            "  Batch   350  of    730.\n",
            "  Batch   400  of    730.\n",
            "  Batch   450  of    730.\n",
            "  Batch   500  of    730.\n",
            "  Batch   550  of    730.\n",
            "  Batch   600  of    730.\n",
            "  Batch   650  of    730.\n",
            "  Batch   700  of    730.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    244.\n",
            "  Batch   100  of    244.\n",
            "  Batch   150  of    244.\n",
            "  Batch   200  of    244.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.316\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    730.\n",
            "  Batch   100  of    730.\n",
            "  Batch   150  of    730.\n",
            "  Batch   200  of    730.\n",
            "  Batch   250  of    730.\n",
            "  Batch   300  of    730.\n",
            "  Batch   350  of    730.\n",
            "  Batch   400  of    730.\n",
            "  Batch   450  of    730.\n",
            "  Batch   500  of    730.\n",
            "  Batch   550  of    730.\n",
            "  Batch   600  of    730.\n",
            "  Batch   650  of    730.\n",
            "  Batch   700  of    730.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    244.\n",
            "  Batch   100  of    244.\n",
            "  Batch   150  of    244.\n",
            "  Batch   200  of    244.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.295\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    730.\n",
            "  Batch   100  of    730.\n",
            "  Batch   150  of    730.\n",
            "  Batch   200  of    730.\n",
            "  Batch   250  of    730.\n",
            "  Batch   300  of    730.\n",
            "  Batch   350  of    730.\n",
            "  Batch   400  of    730.\n",
            "  Batch   450  of    730.\n",
            "  Batch   500  of    730.\n",
            "  Batch   550  of    730.\n",
            "  Batch   600  of    730.\n",
            "  Batch   650  of    730.\n",
            "  Batch   700  of    730.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    244.\n",
            "  Batch   100  of    244.\n",
            "  Batch   150  of    244.\n",
            "  Batch   200  of    244.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.305\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    730.\n",
            "  Batch   100  of    730.\n",
            "  Batch   150  of    730.\n",
            "  Batch   200  of    730.\n",
            "  Batch   250  of    730.\n",
            "  Batch   300  of    730.\n",
            "  Batch   350  of    730.\n",
            "  Batch   400  of    730.\n",
            "  Batch   450  of    730.\n",
            "  Batch   500  of    730.\n",
            "  Batch   550  of    730.\n",
            "  Batch   600  of    730.\n",
            "  Batch   650  of    730.\n",
            "  Batch   700  of    730.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    244.\n",
            "  Batch   100  of    244.\n",
            "  Batch   150  of    244.\n",
            "  Batch   200  of    244.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.345\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.86      0.87      2894\n",
            "           1       0.87      0.89      0.88      2970\n",
            "\n",
            "    accuracy                           0.88      5864\n",
            "   macro avg       0.88      0.88      0.88      5864\n",
            "weighted avg       0.88      0.88      0.88      5864\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    803.\n",
            "  Batch   100  of    803.\n",
            "  Batch   150  of    803.\n",
            "  Batch   200  of    803.\n",
            "  Batch   250  of    803.\n",
            "  Batch   300  of    803.\n",
            "  Batch   350  of    803.\n",
            "  Batch   400  of    803.\n",
            "  Batch   450  of    803.\n",
            "  Batch   500  of    803.\n",
            "  Batch   550  of    803.\n",
            "  Batch   600  of    803.\n",
            "  Batch   650  of    803.\n",
            "  Batch   700  of    803.\n",
            "  Batch   750  of    803.\n",
            "  Batch   800  of    803.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    268.\n",
            "  Batch   100  of    268.\n",
            "  Batch   150  of    268.\n",
            "  Batch   200  of    268.\n",
            "  Batch   250  of    268.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.367\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    803.\n",
            "  Batch   100  of    803.\n",
            "  Batch   150  of    803.\n",
            "  Batch   200  of    803.\n",
            "  Batch   250  of    803.\n",
            "  Batch   300  of    803.\n",
            "  Batch   350  of    803.\n",
            "  Batch   400  of    803.\n",
            "  Batch   450  of    803.\n",
            "  Batch   500  of    803.\n",
            "  Batch   550  of    803.\n",
            "  Batch   600  of    803.\n",
            "  Batch   650  of    803.\n",
            "  Batch   700  of    803.\n",
            "  Batch   750  of    803.\n",
            "  Batch   800  of    803.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    268.\n",
            "  Batch   100  of    268.\n",
            "  Batch   150  of    268.\n",
            "  Batch   200  of    268.\n",
            "  Batch   250  of    268.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.302\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    803.\n",
            "  Batch   100  of    803.\n",
            "  Batch   150  of    803.\n",
            "  Batch   200  of    803.\n",
            "  Batch   250  of    803.\n",
            "  Batch   300  of    803.\n",
            "  Batch   350  of    803.\n",
            "  Batch   400  of    803.\n",
            "  Batch   450  of    803.\n",
            "  Batch   500  of    803.\n",
            "  Batch   550  of    803.\n",
            "  Batch   600  of    803.\n",
            "  Batch   650  of    803.\n",
            "  Batch   700  of    803.\n",
            "  Batch   750  of    803.\n",
            "  Batch   800  of    803.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    268.\n",
            "  Batch   100  of    268.\n",
            "  Batch   150  of    268.\n",
            "  Batch   200  of    268.\n",
            "  Batch   250  of    268.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.301\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    803.\n",
            "  Batch   100  of    803.\n",
            "  Batch   150  of    803.\n",
            "  Batch   200  of    803.\n",
            "  Batch   250  of    803.\n",
            "  Batch   300  of    803.\n",
            "  Batch   350  of    803.\n",
            "  Batch   400  of    803.\n",
            "  Batch   450  of    803.\n",
            "  Batch   500  of    803.\n",
            "  Batch   550  of    803.\n",
            "  Batch   600  of    803.\n",
            "  Batch   650  of    803.\n",
            "  Batch   700  of    803.\n",
            "  Batch   750  of    803.\n",
            "  Batch   800  of    803.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    268.\n",
            "  Batch   100  of    268.\n",
            "  Batch   150  of    268.\n",
            "  Batch   200  of    268.\n",
            "  Batch   250  of    268.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.278\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.88      0.87      2309\n",
            "           1       0.89      0.87      0.88      2388\n",
            "\n",
            "    accuracy                           0.88      4697\n",
            "   macro avg       0.88      0.88      0.88      4697\n",
            "weighted avg       0.88      0.88      0.88      4697\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.302\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.299\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.288\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.268\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.86      0.87      1740\n",
            "           1       0.87      0.90      0.88      1790\n",
            "\n",
            "    accuracy                           0.88      3530\n",
            "   macro avg       0.88      0.88      0.88      3530\n",
            "weighted avg       0.88      0.88      0.88      3530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning for time quantum of 3 months"
      ],
      "metadata": {
        "id": "I4t4H-Bj7LRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3,27,3):\n",
        "  train_text_cov=df1.loc[:int(0.0416*i*0.6*df1.shape[0]),'data'].values\n",
        "  train_labels_cov=df1.loc[:int(0.0416*i*0.6*df1.shape[0]),'label'].values\n",
        "  val_text_cov=df1.loc[int(0.0416*i*0.6*df1.shape[0]):int(0.0416*i*0.8*df1.shape[0]),'data'].values\n",
        "  val_labels_cov=df1.loc[int(0.0416*i*0.6*df1.shape[0]):int(0.0416*i*0.8*df1.shape[0]),\"label\"].values\n",
        "  test_text_cov=df1.loc[int(0.0416*i*0.8*df1.shape[0]):,'data'].values\n",
        "  test_labels_cov=df1.loc[int(0.0416*i*0.8*df1.shape[0]):,'label'].values\n",
        "  max_seq_len = 15\n",
        "  tokens_train = tokenizer.batch_encode_plus(\n",
        "      train_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text_cov.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "  tokens_test = tokenizer.batch_encode_plus(\n",
        "      test_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "  train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "  train_y = torch.tensor(train_labels_cov.tolist())\n",
        "  val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "  val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "  val_y = torch.tensor(val_labels_cov.tolist())\n",
        "  test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "  test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  batch_size = 12\n",
        "  train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "  best_valid_loss = float('inf')\n",
        "  train_losses=[]\n",
        "  valid_losses=[]\n",
        "  epochs=4\n",
        "  for epoch in range(epochs):\n",
        "      print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "      train_loss, _ = train()\n",
        "      valid_loss, _ = evaluate()\n",
        "      if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model3.state_dict(), 'saved_weights3.pt')\n",
        "      train_losses.append(train_loss)\n",
        "      valid_losses.append(valid_loss)\n",
        "      print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "      print(f'Validation Loss: {valid_loss:.3f}')\n",
        "  path = 'saved_weights3.pt'\n",
        "  model3.load_state_dict(torch.load(path))\n",
        "  with torch.no_grad():\n",
        "    preds,_ = model3(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "  preds = np.argmax(preds, axis = 1)\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRYdxWpvNDpZ",
        "outputId": "3a492a83-1327-468e-dfdc-f5d6e116a50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.020\n",
            "Validation Loss: 0.667\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.018\n",
            "Validation Loss: 0.626\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.018\n",
            "Validation Loss: 0.579\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.016\n",
            "Validation Loss: 0.567\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.58      0.68      7844\n",
            "           1       0.68      0.88      0.77      7941\n",
            "\n",
            "    accuracy                           0.73     15785\n",
            "   macro avg       0.76      0.73      0.73     15785\n",
            "weighted avg       0.75      0.73      0.73     15785\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 0.009\n",
            "Validation Loss: 0.473\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 0.008\n",
            "Validation Loss: 0.448\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 0.009\n",
            "Validation Loss: 0.425\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 0.009\n",
            "Validation Loss: 0.423\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.72      0.79      6969\n",
            "           1       0.76      0.90      0.83      7065\n",
            "\n",
            "    accuracy                           0.81     14034\n",
            "   macro avg       0.82      0.81      0.81     14034\n",
            "weighted avg       0.82      0.81      0.81     14034\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    329.\n",
            "  Batch   100  of    329.\n",
            "  Batch   150  of    329.\n",
            "  Batch   200  of    329.\n",
            "  Batch   250  of    329.\n",
            "  Batch   300  of    329.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Training Loss: 0.006\n",
            "Validation Loss: 0.385\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    329.\n",
            "  Batch   100  of    329.\n",
            "  Batch   150  of    329.\n",
            "  Batch   200  of    329.\n",
            "  Batch   250  of    329.\n",
            "  Batch   300  of    329.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Training Loss: 0.006\n",
            "Validation Loss: 0.360\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    329.\n",
            "  Batch   100  of    329.\n",
            "  Batch   150  of    329.\n",
            "  Batch   200  of    329.\n",
            "  Batch   250  of    329.\n",
            "  Batch   300  of    329.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Training Loss: 0.005\n",
            "Validation Loss: 0.352\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    329.\n",
            "  Batch   100  of    329.\n",
            "  Batch   150  of    329.\n",
            "  Batch   200  of    329.\n",
            "  Batch   250  of    329.\n",
            "  Batch   300  of    329.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    110.\n",
            "  Batch   100  of    110.\n",
            "\n",
            "Training Loss: 0.008\n",
            "Validation Loss: 0.496\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.84      0.85      6047\n",
            "           1       0.85      0.87      0.86      6236\n",
            "\n",
            "    accuracy                           0.86     12283\n",
            "   macro avg       0.86      0.86      0.86     12283\n",
            "weighted avg       0.86      0.86      0.86     12283\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.005\n",
            "Validation Loss: 0.342\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.341\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.339\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    438.\n",
            "  Batch   100  of    438.\n",
            "  Batch   150  of    438.\n",
            "  Batch   200  of    438.\n",
            "  Batch   250  of    438.\n",
            "  Batch   300  of    438.\n",
            "  Batch   350  of    438.\n",
            "  Batch   400  of    438.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    146.\n",
            "  Batch   100  of    146.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.324\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.85      0.86      5181\n",
            "           1       0.86      0.88      0.87      5352\n",
            "\n",
            "    accuracy                           0.86     10533\n",
            "   macro avg       0.86      0.86      0.86     10533\n",
            "weighted avg       0.86      0.86      0.86     10533\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    548.\n",
            "  Batch   100  of    548.\n",
            "  Batch   150  of    548.\n",
            "  Batch   200  of    548.\n",
            "  Batch   250  of    548.\n",
            "  Batch   300  of    548.\n",
            "  Batch   350  of    548.\n",
            "  Batch   400  of    548.\n",
            "  Batch   450  of    548.\n",
            "  Batch   500  of    548.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    183.\n",
            "  Batch   100  of    183.\n",
            "  Batch   150  of    183.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.337\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    548.\n",
            "  Batch   100  of    548.\n",
            "  Batch   150  of    548.\n",
            "  Batch   200  of    548.\n",
            "  Batch   250  of    548.\n",
            "  Batch   300  of    548.\n",
            "  Batch   350  of    548.\n",
            "  Batch   400  of    548.\n",
            "  Batch   450  of    548.\n",
            "  Batch   500  of    548.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    183.\n",
            "  Batch   100  of    183.\n",
            "  Batch   150  of    183.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.311\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    548.\n",
            "  Batch   100  of    548.\n",
            "  Batch   150  of    548.\n",
            "  Batch   200  of    548.\n",
            "  Batch   250  of    548.\n",
            "  Batch   300  of    548.\n",
            "  Batch   350  of    548.\n",
            "  Batch   400  of    548.\n",
            "  Batch   450  of    548.\n",
            "  Batch   500  of    548.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    183.\n",
            "  Batch   100  of    183.\n",
            "  Batch   150  of    183.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.304\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    548.\n",
            "  Batch   100  of    548.\n",
            "  Batch   150  of    548.\n",
            "  Batch   200  of    548.\n",
            "  Batch   250  of    548.\n",
            "  Batch   300  of    548.\n",
            "  Batch   350  of    548.\n",
            "  Batch   400  of    548.\n",
            "  Batch   450  of    548.\n",
            "  Batch   500  of    548.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    183.\n",
            "  Batch   100  of    183.\n",
            "  Batch   150  of    183.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.313\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.85      0.87      4289\n",
            "           1       0.86      0.89      0.88      4493\n",
            "\n",
            "    accuracy                           0.87      8782\n",
            "   macro avg       0.87      0.87      0.87      8782\n",
            "weighted avg       0.87      0.87      0.87      8782\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    657.\n",
            "  Batch   100  of    657.\n",
            "  Batch   150  of    657.\n",
            "  Batch   200  of    657.\n",
            "  Batch   250  of    657.\n",
            "  Batch   300  of    657.\n",
            "  Batch   350  of    657.\n",
            "  Batch   400  of    657.\n",
            "  Batch   450  of    657.\n",
            "  Batch   500  of    657.\n",
            "  Batch   550  of    657.\n",
            "  Batch   600  of    657.\n",
            "  Batch   650  of    657.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.338\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    657.\n",
            "  Batch   100  of    657.\n",
            "  Batch   150  of    657.\n",
            "  Batch   200  of    657.\n",
            "  Batch   250  of    657.\n",
            "  Batch   300  of    657.\n",
            "  Batch   350  of    657.\n",
            "  Batch   400  of    657.\n",
            "  Batch   450  of    657.\n",
            "  Batch   500  of    657.\n",
            "  Batch   550  of    657.\n",
            "  Batch   600  of    657.\n",
            "  Batch   650  of    657.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.334\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    657.\n",
            "  Batch   100  of    657.\n",
            "  Batch   150  of    657.\n",
            "  Batch   200  of    657.\n",
            "  Batch   250  of    657.\n",
            "  Batch   300  of    657.\n",
            "  Batch   350  of    657.\n",
            "  Batch   400  of    657.\n",
            "  Batch   450  of    657.\n",
            "  Batch   500  of    657.\n",
            "  Batch   550  of    657.\n",
            "  Batch   600  of    657.\n",
            "  Batch   650  of    657.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.324\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    657.\n",
            "  Batch   100  of    657.\n",
            "  Batch   150  of    657.\n",
            "  Batch   200  of    657.\n",
            "  Batch   250  of    657.\n",
            "  Batch   300  of    657.\n",
            "  Batch   350  of    657.\n",
            "  Batch   400  of    657.\n",
            "  Batch   450  of    657.\n",
            "  Batch   500  of    657.\n",
            "  Batch   550  of    657.\n",
            "  Batch   600  of    657.\n",
            "  Batch   650  of    657.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    219.\n",
            "  Batch   100  of    219.\n",
            "  Batch   150  of    219.\n",
            "  Batch   200  of    219.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.336\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.80      0.85      3453\n",
            "           1       0.83      0.93      0.88      3578\n",
            "\n",
            "    accuracy                           0.87      7031\n",
            "   macro avg       0.87      0.87      0.87      7031\n",
            "weighted avg       0.87      0.87      0.87      7031\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    766.\n",
            "  Batch   100  of    766.\n",
            "  Batch   150  of    766.\n",
            "  Batch   200  of    766.\n",
            "  Batch   250  of    766.\n",
            "  Batch   300  of    766.\n",
            "  Batch   350  of    766.\n",
            "  Batch   400  of    766.\n",
            "  Batch   450  of    766.\n",
            "  Batch   500  of    766.\n",
            "  Batch   550  of    766.\n",
            "  Batch   600  of    766.\n",
            "  Batch   650  of    766.\n",
            "  Batch   700  of    766.\n",
            "  Batch   750  of    766.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    256.\n",
            "  Batch   100  of    256.\n",
            "  Batch   150  of    256.\n",
            "  Batch   200  of    256.\n",
            "  Batch   250  of    256.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.308\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    766.\n",
            "  Batch   100  of    766.\n",
            "  Batch   150  of    766.\n",
            "  Batch   200  of    766.\n",
            "  Batch   250  of    766.\n",
            "  Batch   300  of    766.\n",
            "  Batch   350  of    766.\n",
            "  Batch   400  of    766.\n",
            "  Batch   450  of    766.\n",
            "  Batch   500  of    766.\n",
            "  Batch   550  of    766.\n",
            "  Batch   600  of    766.\n",
            "  Batch   650  of    766.\n",
            "  Batch   700  of    766.\n",
            "  Batch   750  of    766.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    256.\n",
            "  Batch   100  of    256.\n",
            "  Batch   150  of    256.\n",
            "  Batch   200  of    256.\n",
            "  Batch   250  of    256.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.304\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    766.\n",
            "  Batch   100  of    766.\n",
            "  Batch   150  of    766.\n",
            "  Batch   200  of    766.\n",
            "  Batch   250  of    766.\n",
            "  Batch   300  of    766.\n",
            "  Batch   350  of    766.\n",
            "  Batch   400  of    766.\n",
            "  Batch   450  of    766.\n",
            "  Batch   500  of    766.\n",
            "  Batch   550  of    766.\n",
            "  Batch   600  of    766.\n",
            "  Batch   650  of    766.\n",
            "  Batch   700  of    766.\n",
            "  Batch   750  of    766.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    256.\n",
            "  Batch   100  of    256.\n",
            "  Batch   150  of    256.\n",
            "  Batch   200  of    256.\n",
            "  Batch   250  of    256.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.296\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    766.\n",
            "  Batch   100  of    766.\n",
            "  Batch   150  of    766.\n",
            "  Batch   200  of    766.\n",
            "  Batch   250  of    766.\n",
            "  Batch   300  of    766.\n",
            "  Batch   350  of    766.\n",
            "  Batch   400  of    766.\n",
            "  Batch   450  of    766.\n",
            "  Batch   500  of    766.\n",
            "  Batch   550  of    766.\n",
            "  Batch   600  of    766.\n",
            "  Batch   650  of    766.\n",
            "  Batch   700  of    766.\n",
            "  Batch   750  of    766.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    256.\n",
            "  Batch   100  of    256.\n",
            "  Batch   150  of    256.\n",
            "  Batch   200  of    256.\n",
            "  Batch   250  of    256.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.292\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.85      0.87      2615\n",
            "           1       0.86      0.90      0.88      2666\n",
            "\n",
            "    accuracy                           0.87      5281\n",
            "   macro avg       0.87      0.87      0.87      5281\n",
            "weighted avg       0.87      0.87      0.87      5281\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.339\n",
            "\n",
            " Epoch 2 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.280\n",
            "\n",
            " Epoch 3 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.310\n",
            "\n",
            " Epoch 4 / 4\n",
            "  Batch    50  of    876.\n",
            "  Batch   100  of    876.\n",
            "  Batch   150  of    876.\n",
            "  Batch   200  of    876.\n",
            "  Batch   250  of    876.\n",
            "  Batch   300  of    876.\n",
            "  Batch   350  of    876.\n",
            "  Batch   400  of    876.\n",
            "  Batch   450  of    876.\n",
            "  Batch   500  of    876.\n",
            "  Batch   550  of    876.\n",
            "  Batch   600  of    876.\n",
            "  Batch   650  of    876.\n",
            "  Batch   700  of    876.\n",
            "  Batch   750  of    876.\n",
            "  Batch   800  of    876.\n",
            "  Batch   850  of    876.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    292.\n",
            "  Batch   100  of    292.\n",
            "  Batch   150  of    292.\n",
            "  Batch   200  of    292.\n",
            "  Batch   250  of    292.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.281\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.88      0.87      1740\n",
            "           1       0.88      0.87      0.87      1790\n",
            "\n",
            "    accuracy                           0.87      3530\n",
            "   macro avg       0.87      0.87      0.87      3530\n",
            "weighted avg       0.87      0.87      0.87      3530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning for a time quantum of 4 months"
      ],
      "metadata": {
        "id": "z-2Lplk4_SkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4,28,4):\n",
        "  train_text_cov=df1.loc[:int(0.0416*i*0.6*df1.shape[0]),'data'].values\n",
        "  train_labels_cov=df1.loc[:int(0.0416*i*0.6*df1.shape[0]),'label'].values\n",
        "  val_text_cov=df1.loc[int(0.0416*i*0.6*df1.shape[0]):int(0.0416*i*0.8*df1.shape[0]),'data'].values\n",
        "  val_labels_cov=df1.loc[int(0.0416*i*0.6*df1.shape[0]):int(0.0416*i*0.8*df1.shape[0]),\"label\"].values\n",
        "  test_text_cov=df1.loc[int(0.0416*i*0.8*df1.shape[0]):,'data'].values\n",
        "  test_labels_cov=df1.loc[int(0.0416*i*0.8*df1.shape[0]):,'label'].values\n",
        "  max_seq_len = 15\n",
        "  tokens_train = tokenizer.batch_encode_plus(\n",
        "      train_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text_cov.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "  tokens_test = tokenizer.batch_encode_plus(\n",
        "      test_text_cov.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "  train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "  train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "  train_y = torch.tensor(train_labels_cov.tolist())\n",
        "  val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "  val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "  val_y = torch.tensor(val_labels_cov.tolist())\n",
        "  test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "  test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  batch_size = 12\n",
        "  train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "  best_valid_loss = float('inf')\n",
        "  train_losses=[]\n",
        "  valid_losses=[]\n",
        "  epochs=4\n",
        "  for epoch in range(epochs):\n",
        "      print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "      train_loss, _ = train()\n",
        "      valid_loss, _ = evaluate()\n",
        "      if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model4.state_dict(), 'saved_weights4.pt')\n",
        "      train_losses.append(train_loss)\n",
        "      valid_losses.append(valid_loss)\n",
        "      print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "      print(f'Validation Loss: {valid_loss:.3f}')\n",
        "  path = 'saved_weights4.pt'\n",
        "  model4.load_state_dict(torch.load(path))\n",
        "  with torch.no_grad():\n",
        "    preds,_ = model4(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "  preds = np.argmax(preds, axis = 1)\n",
        "  test_y = torch.tensor(test_labels_cov.tolist())\n",
        "  print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "id": "IyXFdoXs_MuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning for a time quantum of 1 month"
      ],
      "metadata": {
        "id": "NW_Mn4VI7fp9"
      }
    }
  ]
}